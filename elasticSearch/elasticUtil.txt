No entanto, Elasticsearch é muito mais do que apenas Lucene e muito mais do que “apenas” pesquisa de texto completo. Também pode ser descrito da seguinte forma:

- Um armazenamento de documentos em tempo real distribuído onde cada campo é indexado e pesquisável

- Um mecanismo de pesquisa distribuído com análises em tempo real

- Capaz de escalar para centenas de servidores e petabytes de dados estruturados e não estruturados


Elasticsearch é orientado a documentos , o que significaque armazena objetos ou documentos inteiros. Ele não apenas os armazena, mas também indexa o conteúdo de cada documento para torná-los pesquisáveis.


--Métodos simples de consulta:

Para fazer isso, usaremos um método de pesquisa leve que é fácil de usar na linha de comando. Este método é frequentemente referido comouma pesquisa de string de consulta , já que passamos a pesquisa como um parâmetro de string de consulta de URL:

GET /megacorp/employee/_search?q=last_name:Smith

Elasticsearch fornece uma linguagem de consulta rica e flexível chamada DSL de consulta , que nos permite construir consultas muito mais complexas e robustas.

--Consulta com dsl

O idioma específico do domínio (DSL) éespecificado usando um corpo de solicitação JSON. Podemos representar a pesquisa anterior para todos os Smiths da seguinte forma:

GET /megacorp/employee/_search
{
    "query" : {
        "match" : {
            "last_name" : "Smith"
        }
    }
}

-- Subir elastic no docker --

docker run -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:6.8.19

----

Vamos complicar um pouco a pesquisa. Ainda queremos encontrar todos os funcionários com o sobrenome Smith, mas queremos apenas funcionários com mais de 30 anos.
Nossa consulta mudará um pouco para acomodar um filtro , que nos permite executar pesquisas estruturadas com eficiência:

{
    "query" : {
        "filtered" : {
            "filter" : {
                "range" : {
                    "age" : { "gt" : 30 } 1
                }
            },
            "query" : {
                "match" : {
                    "last_name" : "smith" 2
                }
            }
        }
    }
}

 -- Match trata parecido por exemplo buscar que gosta de escalada:

{
"query" : {
        "match" : {
            "about" : "rock climbing"
        }
    }
}

Irá retornar quem curte rock tbm... para contornarmos isso é preciso buscar a palavra igual por meio do:

"match_phrase" : {
            "about" : "rock climbing"
        }


-- Destacando nossas pesquisas --



{
  "query" : {
    "match_phrase" : {
      "about" : "rock climbing"
    }
  },
  "highlight": {
    "fields" : {
      "about" : {}
    }
  }
}

Dessa forma vai trazer o le contém um trecho de texto do aboutcampo com as palavras correspondentes embrulhadas em <em></em> tags HTML.

--Analytics

Elasticsearch tem uma funcionalidade chamada agregações , quepermitem que você gere análises sofisticadas sobre seus dados.
É semelhante ao GROUP BY SQL, mas muito mais poderoso.



Essas agregações não são pré-calculadas; eles são gerados instantaneamente a partir dos documentos que correspondem à consulta atual


--Natureza distribuida

Embora nosso tutorial tenha dado exemplos de como usar o Elasticsearch, ele não tocou na mecânica.
Elasticsearch é distribuído por natureza e foi projetado para ocultar a complexidade que vem com a distribuição.

Elasticsearch se esforça para esconder a complexidade dos sistemas distribuídos.
Aqui estão algumas das operações que acontecem automaticamente nos bastidores:

Capítulo 3. Entrada de Dados, Saída de Dados

Elasticsearch é um armazenamento de documentos distribuído .Ele pode armazenar e recuperar estruturas de dados complexas - serializadas como documentos JSON - em tempo real .
Em outras palavras, assim que um documento é armazenado no Elasticsearch, ele pode ser recuperado de qualquer nó do cluster.

No Elasticsearch, todos os dados em cada campo são indexados por padrão .Ou seja, cada campo tem um índice invertido dedicado para recuperação rápida.

Um documento não consiste apenas em seus dados.Ele também tem metadados -informações sobre o documento.
Os três elementos de metadados necessários são os seguintes:

_index
Onde o documento mora

_type
A classe de objeto que o documento representa

_id
O identificador único do documento

Nome do índice: Este nome deve estar em minúsculas, não pode começar com sublinhado e não pode conter vírgulas.
Vamos usar websitecomo nosso nome de índice.

Por exemplo, se nosso índice for chamado website, nosso tipo for chamado bloge escolhermos o ID 123, a solicitação de índice terá a seguinte aparência:

PUT /website/blog/123
{
  "title": "My first blog entry",
  "text":  "Just trying this out...",
  "date":  "2014/01/01"
}


IDs de geração automática
Se nossos dados não tiverem um ID natural, podemos deixar o Elasticsearch gerar um para nós automaticamente. A estrutura da solicitação muda:
em vez de usar o PUT verbo ( “guardar este documento nesta URL”), usamos o POSTverbo ( “guardar este documento sob este URL”).

Exemplo:

POST /website/blog/
{
  "title": "My second blog entry",
  "text":  "Still trying this out...",
  "date":  "2014/01/01"
}

Recuperando um Documento:

GET / site / blog / 123? pretty

Adicionando prettyaos parâmetros de string de consulta para qualquer solicitação,
como no exemplo anterior, faz com que Elasticsearch imprima bem oResposta JSON para torná-lo mais legível.

Recuperando Parte de um Documento:

Ou se quiser apenas o _sourcecampo sem metadados, você pode usar o _sourceendpoint.

GET / website / blog / 123 / _source

Verificar se existe um documento

Se tudo o que você deseja fazer é verificar se um documento existe...

curl -i -XHEAD http://localhost:9200/website/blog/123

retorna um 200 se assim o doc existir, se não existir retorna

---------------Atualizando um documento inteiro---------------

Os documentos no Elasticsearch são imutáveis ; não podemos mudá-los.Em vez disso, se precisarmos atualizar um documento existente,
nós o reindexamos ou substituímos,que podemos fazer usando a mesma index.

Exemplo atualização

PUT /website/blog/123
{
  "title": "My first blog entry",
  "text":  "I am starting to get the hang of this...",
  "date":  "2014/01/02"
}


Internamente, o Elasticsearch marcou o documento antigo como excluído e adicionou um documento inteiramente novo.
A versão antiga do documento não desaparece imediatamente, embora você não consiga acessá-la.
O Elasticsearch limpa os documentos excluídos em segundo plano à medida que você continua a indexar mais dados.

---------------Criando um Novo Documento---------------

No entanto, se já temos um _idque queremos usar, então temos de dizer ElasticSearch que ele deve aceitar o nosso pedido índice apenas se um documento com o mesmo _index, _typee _idainda não existir.

Exemplo de chamada:

PUT /website/blog/123?op_type=create
{ ... }

OU

PUT /website/blog/123/_create
{ ... }

Caso existir retorna um 409

{
  "error" : "DocumentAlreadyExistsException[[website][4] [blog][123]:
             document already exists]",
  "status" : 409
}

------------Atualizações parciais de documentos------

O updateAPI deve obedecer às mesmas regras. Externamente, parece que estamos atualizando parcialmente um documento no local.
Internamente, no entanto, a updateAPI simplesmente gerencia o mesmo processo de recuperação-alteração-reindexação que já descrevemos.

A forma mais simples da updatesolicitação aceita um documento parcial como docparâmetro, que apenas é mesclado com o documento existente.

Exemplo:

POST /website/blog/1/_update
{
   "doc" : {
      "tags" : [ "testing" ],
      "views": 0
   }
}

---------------Usando scripts para fazer atualizações parciais---------------

Para aqueles momentos em que a API não é suficiente, o Elasticsearch permite que você escreva sua própria lógica personalizada em um script.
O script é compatível com muitas APIs, incluindo pesquisa, classificação, agregações e atualizações de documentos.
Os scripts podem ser passados como parte da solicitação, recuperados do .scripts índice especial ou carregados do disco.
A linguagem de script padrão é Groovy.


---------------Excluindo um Documento---------------

A sintaxe para excluir um documento segue o mesmo padrão que já vimos, mas usa o DELETE método:

DELETE /website/blog/123

-----Recuperando vários documentos-----
Por mais rápido que o Elasticsearch seja, ele pode ser ainda mais rápido.Combinar várias solicitações em uma evita a sobrecarga da rede de processar cada solicitação individualmente.
Se você sabe que precisa recuperar vários documentos do Elasticsearch,
é mais rápido recuperá-los todos em uma única solicitação usando o multi-get , ou mgetAPI, em vez de documento por documento.

A mgetAPI espera uma docsmatriz, cadaelemento que especifica o _index... Exemplo:

GET /_mget
{
   "docs" : [
      {
         "_index" : "website",
         "_type" :  "blog",
         "_id" :    2
      },
      {
         "_index" : "website",
         "_type" :  "pageviews",
         "_id" :    1,
         "_source": "views"
      }
   ]
}

O corpo da resposta também contém uma docsmatrizque contém uma resposta por documento...

{
   "docs" : [
      {
         "_index" :   "website",
         "_id" :      "2",
         "_type" :    "blog",
         "found" :    true,
         "_source" : {
            "text" :  "This is a piece of cake...",
            "title" : "My first external blog entry"
         },
         "_version" : 10
      },
      {
         "_index" :   "website",
         "_id" :      "1",
         "_type" :    "pageviews",
         "found" :    true,
         "_version" : 2,
         "_source" : {
            "views" : 2
         }
      }
   ]
}




---Lidando com Conflitos---

Ao atualizar um documento com a indexAPI,
lemos o documento original, fazemos nossas alterações e, em seguida, reindexamos todo o documento de uma vez.


No mundo do banco de dados, duas abordagens são comumente usadas para garantir que as mudanças não sejam perdidas ao fazer atualizações simultâneas:

Controle de concorrência pessimista
Amplamente usado por bancos de dados relacionais, essa abordagem pressupõe que mudanças conflitantes podem acontecer e,
portanto, bloqueia o acesso a um recurso para evitar conflitos.
Um exemplo típico é bloquear uma linha antes de ler seus dados, garantindo que apenas o encadeamento que colocou o bloqueio seja capaz de fazer alterações nos dados dessa linha.

Controle de concorrência otimista
Usado por Elasticsearch, essa abordagem pressupõe que é improvável que os conflitos ocorram e não impede que as operações sejam tentadas.
No entanto, se os dados subjacentes foram modificados entre a leitura e a gravação, a atualização falhará. Cabe então ao aplicativo decidir como deve resolver o conflito.
Por exemplo, ele pode tentar novamente a atualização, usando os dados novos, ou pode relatar a situação ao usuário.


Um _version número que é incrementado sempre que um documento é alterado.
Elasticsearch usa esse _versionnúmero para garantir que as alterações sejam aplicadas na ordem correta.
Se uma versão mais antiga de um documento chegar após uma nova versão, ela pode ser simplesmente ignorada.

---Controle de simultaneidade otimista*--


Vamos criar uma nova postagem no blog:

PUT /website/blog/1/_create
{
  "title": "My first blog entry",
  "text":  "Just trying this out..."
}

O corpo da resposta nos diz que este documento recém-criado possui _version número 1.

Agora, quando tentamos salvar nossas alterações reindexando o documento, especificamos o versionao qual nossas alterações devem ser aplicadas:


PUT /website/blog/1?version=1 1
{
  "title": "My first blog entry",
  "text":  "Starting to get the hang of this..."
}

Essa solicitação é bem-sucedida e o corpo da resposta nos diz que o _version foi incrementado para 2...


No entanto, se tivéssemos de executar novamente a mesma solicitação de índice, ainda especificando version=1, Elasticsearch responderia com um 409 Conflictcódigo de resposta HTTP.

Isso nos diz que o _version número atual do documento no Elasticsearch é 2, mas que especificamos que estávamos atualizando a versão 1.

Todas as APIs que atualizam ou excluem um documento aceitam um version parâmetro, o que permite que você aplique o controle de simultaneidade otimista apenas às partes do seu código onde faz sentido.


--------Usando versões de um sistema externo-----

Se seu banco de dados principal já tiver números de versão - ou um valor como timestampesse pode ser usado como um número de versão - você pode reutilizar esses mesmos números de versão no Elasticsearch adicionando version_type=externalà string de consulta.
Os números da versão devem ser inteiros maiores que zero e menores que cerca de 9.2e+18- um longvalor positivo em Java.

Como o elastic verifica:

Em vez de verificar se o atual _versioné o mesmo especificado na solicitação, o Elasticsearch verifica se o atual _versioné menor que a versão especificada.

**Os números de versão externa podem ser especificados não apenas nas solicitações de indexação e exclusão, mas também na criação de novos documentos.

Exemplo de criação com external:

PUT /website/blog/2?version=5&version_type=external
{
  "title": "My first external blog entry",
  "text":  "Starting to get the hang of this..."
}

Agora, atualizamos este documento, especificando um novo versionnúmero de 10:

PUT /website/blog/2?version=10&version_type=external
{
  "title": "My first external blog entry",
  "text":  "This is a piece of cake..."
}
**Se você executasse novamente esta solicitação, ela falharia com o mesmo erro de conflito que vimos antes, porque o número da versão externa especificada não é maior do que a versão atual no Elasticsearch.
Dessa forma vai atualizando de acordo com o que é passado na version external.

---Atualizações parciais de documentos

Também dissemos que os documentos são imutáveis: não podem ser alterados, apenas substituídos. A updateAPI deve obedecer às mesmas regras.
Externamente, parece que estamos atualizando parcialmente um documento no local.
Internamente, no entanto, a updateAPI simplesmente gerencia o mesmo processo de recuperação-alteração-reindexação que já descrevemos

A forma mais simples da update solicitação aceita um documento parcial como docparâmetro, que apenas é mesclado com o documento existente.

Por exemplo, poderíamos adicionar um tagscampo e um viewscampo à postagem do nosso blog da seguinte maneira:

POST /website/blog/1/_update
{
   "doc" : {
      "tags" : [ "testing" ],
      "views": 0
   }
}

------Recuperando vários documentos


A mgetAPI espera uma docsmatriz, cadaelemento que especifica o _index, _typee _id metadados do documento que você deseja recuperar.
Você também pode especificar um _source parâmetro se quiser apenas recuperar um ou mais campos específicos:

GET /_mget
{
   "docs" : [
      {
         "_index" : "website",
         "_type" :  "blog",
         "_id" :    2
      },
      {
         "_index" : "website",
         "_type" :  "pageviews",
         "_id" :    1,
         "_source": "views"
      }
   ]
}

O corpo da resposta também contém uma docs matriz que contém uma resposta por documento, na mesma ordem especificada na solicitação,
resposta que esperaríamos de uma get solicitação individual...

Exemplo chamada com vários documentos:

Na verdade, se todos os documentos tiverem o mesmo _indexe _type, você poderá simplesmente passar uma matriz de em idsvez da docsmatriz completa :

GET /website/blog/_mget
{
   "ids" : [ "2", "1" ]
}

Observe que o segundo documento que solicitamos não existe.
Especificamos o tipo blog, mas o documento com ID 1é do tipo pageviews. Essa inexistência é relatada no corpo da resposta:


{
  "docs" : [
    {
      "_index" :   "website",
      "_type" :    "blog",
      "_id" :      "2",
      "_version" : 10,
      "found" :    true,
      "_source" : {
        "title":   "My first external blog entry",
        "text":    "This is a piece of cake..."
      }
    },
    {
      "_index" :   "website",
      "_type" :    "blog",
      "_id" :      "1",
      "found" :    false  1
    }
  ]
}

O código de status HTTP para a solicitação anterior é 200, embora um documento não tenha sido encontrado.
Na verdade, ainda seria 200 se nenhum dos documentos solicitados fosse encontrado -  porque a mget própria solicitação foi concluída com êxito.
Para determinar o sucesso ou falha dos documentos individuais, você precisa verificara found bandeira.


------------Bulk------------------

Exemplo de bulk...

POST /_bulk
{ "delete": { "_index": "website", "_type": "blog", "_id": "123" }} 1
{ "create": { "_index": "website", "_type": "blog", "_id": "123" }}
{ "title":    "My first blog post" }
{ "index":  { "_index": "website", "_type": "blog" }}
{ "title":    "My second blog post" }
{ "update": { "_index": "website", "_type": "blog", "_id": "123", "_retry_on_conflict" : 3} }
{ "doc" : {"title" : "My updated blog post"} }


Observe como a deleteação não possui um corpo de solicitação; ele é seguido imediatamente por outra ação.

A resposta do Elasticsearch contém a itemsmatriz, que lista o resultado de cada solicitação, na mesma ordem em que os solicitamos:

{
   "took": 4,
   "errors": false, <<<< indica que todos foram executados com sucesso!
   "items": [
      {  "delete": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "_version": 2,
            "status":   200,
            "found":    true
      }},
      {  "create": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "_version": 3,
            "status":   201
      }},
      {  "create": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "EiwfApScQiiy7TIKFxRCTw",
            "_version": 1,
            "status":   201
      }},
      {  "update": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "_version": 4,
            "status":   200
      }}
   ]
}}



IMPORTANTE ---> Cada sub-solicitação é executada de forma independente, portanto, a falha de uma sub-solicitação não afetará o sucesso das outras.

Se alguma das solicitações falhar, a errorsinalização de nível superior será definida como true

Exemplo com erro:

POST /_bulk
{ "create": { "_index": "website", "_type": "blog", "_id": "123" }}
{ "title":    "Cannot create - it already exists" }
{ "index":  { "_index": "website", "_type": "blog", "_id": "123" }}
{ "title":    "But we can update it" }

Na resposta, podemos ver que não foi possível create documentar 123 porque já existe, mas a index solicitação subsequente , também no documento 123, foi bem-sucedida:

{
   "took": 3,
   "errors": true, <<<< indica que teve erro
   "items": [
      {  "create": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "status":   409, <<< solicitacao com erro
            "error":    "DocumentAlreadyExistsException <<<< exception
                        [[website][4] [blog][123]:
                        document already exists]"
      }},
      {  "index": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "_version": 5,
            "status":   200 <<<< outra requisição foi OK!
      }}
   ]
}

IMPORTANTEEEEEEE:

Isso também significa que os bulk pedidos não são atômicos: eles não podem ser usados para implementar transações.
Cada solicitação é processada separadamente, portanto, o sucesso ou a falha de uma solicitação não interfere nas outras.


Talvez você esteja indexando dados de registro de indexação em lote no mesmo indexe com o mesmo type.
Ter deespecificar os mesmos metadados para cada documento é um desperdício.
Em vez disso, assim como para a mgetAPI, a bulksolicitação aceita um padrão /_indexou /_index/_typeno URL:


--Exemplo bulk para o mesmo indice--

POST /website/log/_bulk
{ "index": {}}
{ "event": "User logged in" }
{ "index": { "_type": "blog" }}
{ "title": "Overriding the default type" }


Toda a solicitação em massa precisa ser carregada na memória pelo nó que recebe nossa solicitação,
portanto, quanto maior a solicitação, menos memória disponível para outras solicitações...

Existe um tamanho ideal de solicitação em massa. Acima desse tamanho,
o desempenho não melhora mais e pode até cair. O tamanho ideal, entretanto, não é um número fixo.

Felizmente, é fácil encontrar este ponto ideal: tente indexar documentos típicos em lotes de tamanho crescente.
Quando o desempenho começa a cair, o tamanho do lote é muito grande.
Um bom lugar para começar é com lotes de 1.000 a 5.000 documentos ou, se os seus documentos forem muito grandes, com lotes ainda menores.

Muitas vezes, é útil ficar de olho no tamanho físico de suas solicitações em massa.
Mil documentos de 1 KB são muito diferentes de mil documentos de 1 MB. Um bom tamanho em massa para começar a jogar é em torno de 5 a 15 MB.

-------Encaminhando um documento para um fragmento----

Quando você indexa um documento, ele é armazenado em um único fragmento primário.Como o Elasticsearch sabe a qual fragmento um documento pertence?
Quando criamos um novo documento, como ele sabe se deve armazenar esse documento no fragmento 1 ou no fragmento 2?

O processo não pode ser aleatório, pois podemos precisar recuperar o documento no futuro. Na verdade, é determinado por uma fórmula simples:

shard = hash (roteamento)% number_of_primary_shards

Todas as APIs de documentos ( get, index, delete, bulk, update, e mget) aceitar um routingparâmetroque pode ser usado para personalizar o mapeamento de documento para fragmento.
Um valor de roteamento personalizado pode ser usado para garantir que todos os documentos relacionados - por exemplo,
todos os documentos pertencentes ao mesmo usuário - sejam armazenados no mesmo fragmento


--------Padrões de documentos múltiplos-------------

Os padrões para as APIs mget e bulk são semelhantes aos de documentos individuais. A diferença é que o nó solicitante sabe em qual fragmento reside cada documento.
Ele divide a solicitação de vários documentos em uma solicitação de vários documentos por fragmento e os encaminha em paralelo para cada nó participante.

--------Por que o formato engraçado?--------

Por que a bulk API exige o formato engraçado com os caracteres de nova linha, em vez de apenas enviar as solicitações agrupadas em uma matriz JSON, como a mget API?

Se as solicitações individuais fossem agrupadas em uma matriz JSON, isso significaria que precisaríamos fazer o seguinte:

Analise o JSON em uma matriz (incluindo os dados do documento, que podem ser muito grandes)

Observe cada solicitação para determinar para qual fragmento ela deve ir

Crie uma série de solicitações para cada fragmento

Serialize essas matrizes no formato de transporte interno

Envie os pedidos para cada fragmento

Funcionaria, mas precisaria de muita RAM para manter cópias essencialmente dos mesmos dados e criaria muito mais estruturas de dados que a Java Virtual Machine (JVM) teria para gastar tempo coletando lixo.

Em vez disso, o Elasticsearch alcança o buffer de rede, onde a solicitação bruta foi recebida, e lê os dados diretamente. Ele usa os caracteres de nova linha para identificar e analisar apenas as pequenas action/metadatalinhas a fim de decidir qual fragmento deve lidar com cada solicitação.

Essas solicitações brutas são encaminhadas diretamente para o fragmento correto. Não há cópia redundante de dados, nem estruturas de dados perdidas. Todo o processo de solicitação é tratado na menor quantidade de memória possível.

-----------------Buscas elasticSearch---------------------------------


Até agora, aprendemos como usar o Elasticsearch como um armazenamento simples de documentos distribuídos no estilo NoSQL.
Podemos jogar documentos JSON no Elasticsearch e recupere cada um por ID.
Mas o verdadeiro poder do Elasticsearch está em sua capacidade de extrair sentido do caos - transformar Big Data em Big Information.

Esse é o motivo pelo qual usamos documentos JSON estruturados, em vez de blobs amorfos de dados.
Elasticsearch não apenas armazena o documento, mas também indexa o conteúdo do documento para torná-lo pesquisável.

Cada campo em um documento é indexado e pode ser consultado. E não é só isso.
Durante uma única consulta, o Elasticsearch pode usar todos esses índices para retornar resultados

Embora muitas pesquisas funcionem apenas a toque de caixa, para usar o Elasticsearch em todo o seu potencial, você precisa entender três assuntos:

Mapeamento:
    Como os dados em cada campo são interpretados

Análise:
    Como o texto completo é processado para torná-lo pesquisável

Consultar DSL:
    A linguagem de consulta poderosa e flexível usada pelo Elasticsearch

--> Transformar em elasticSample.http: https://gist.github.com/clintongormley/8579281



Capítulo 5. Pesquisando - As ferramentas básicas
Até agora, aprendemos como usar o Elasticsearch como um armazenamento simples de documentos distribuídos no estilo NoSQL. Podemosjogue documentos JSON no Elasticsearch e recupere cada um por ID. Mas o verdadeiro poder do Elasticsearch está em sua capacidade de extrair sentido do caos - transformar Big Data em Big Information.

Esse é o motivo pelo qual usamos documentos JSON estruturados, em vez de blobs amorfos de dados. Elasticsearch não apenas armazena o documento, mas também indexa o conteúdo do documento para torná-lo pesquisável.

Cada campo em um documento é indexado e pode ser consultado .E não é só isso. Durante uma única consulta, o Elasticsearch pode usar todos esses índices para retornar resultados em uma velocidade de tirar o fôlego. Isso é algo que você nunca poderia considerar fazer com um banco de dados tradicional.

Uma pesquisa pode ser qualquer uma das seguintes:

Uma consulta estruturada em campos concretoscomo genderou age, classificado por um campo como join_date, semelhante ao tipo de consulta que você poderia construir em SQL

Uma consulta de texto completo, que encontra todos os documentos que correspondem às palavras-chave da pesquisa e os retorna classificados por relevância

Uma combinação dos dois

Embora muitas pesquisas funcionem apenas a partir de a caixa, para usar o Elasticsearch em todo o seu potencial, você precisa entender três assuntos:

Mapeamento
Como os dados em cada campo são interpretados

Análise
Como o texto completo é processado para torná-lo pesquisável

Consultar DSL
A linguagem de consulta poderosa e flexível usada pelo Elasticsearch

Cada um deles é um grande assunto por si só, e os explicamos em detalhes na Parte II . Os capítulos desta seção apresentam os conceitos básicos de todos os três - apenas o suficiente para ajudá-lo a obter uma compreensão geral de como a pesquisa funciona.

Começaremos explicando a searchAPI em sua forma mais simples.

DADOS DE TESTE
Os documentos que usaremos para fins de teste neste capítulo podem ser encontrados neste resumo: https://gist.github.com/clintongormley/8579281 .

Você pode copiar os comandos e colá-los em seu shell para acompanhar este capítulo.

Alternativamente, se você estiver na versão online deste livro, você pode clicar aqui para abrir no Sense ( sense_widget.html? Snippets / 050_Search / Test_data.json ).

------------------A Busca Vazia------------------

A forma mais básica da API de pesquisa é a pesquisa vazia, que não especifica nenhuma consulta, mas simplesmente retorna todos os documentos em todos os índices do cluster:

GET /_search

A seção mais importante da resposta é hits, quecontém o totalnúmero de documentos que corresponderam à
nossa consulta e uma hits matriz contendo os 10 primeiros desses documentos correspondentes - os resultados.

Cada elemento também tem um _score. Essa é a pontuação de relevância , que é uma medida de quão bem o documento corresponde à consulta.
São retornados primeiro com os documentos mais relevantes; ou seja, em ordem decrescente de _score;
O tookvalor nos diz quantos milissegundos toda a solicitação de pesquisa;


O timed_out valor diznos informa se a consulta expirou. Exemplo:

GET /_search?timeout=10ms


---------------Multi-índice, Multitipo-------------

Ao não limitar nossa pesquisa a um índice ou tipo específico, pesquisamos em todos os documentos do cluster. O Elasticsearch encaminhou a solicitação de pesquisa em
paralelo para um primário ou réplica de cada shard no cluster, reuniu os resultados para selecionar os 10 principais gerais e os retornou para nós.

Normalmente, no entanto, você vai desejar pesquisar em um ou mais índices específicos e provavelmente em um ou mais tipos específicos.
Podemos fazer isso especificando o índice e digitando a URL, da seguinte forma:


/_search
Pesquise todos os tipos em todos os índices

/gb/_search
Pesquisar todos os tipos no gbíndice

/gb,us/_search
Pesquise todos os tipos nos índices gbeus

/g*,u*/_search
Pesquise todos os tipos em qualquer índice começando com gou começando comu

/gb/user/_search
Tipo de pesquisa userno gbíndice

/gb,us/user,tweet/_search
Tipos de pesquisa usere tweetnos índices gbeus

/_all/user,tweet/_search
Tipos de pesquisa usere tweetem todos os índices

---------------Paginação---------------

Da mesma forma que o SQL usa a LIMIT palavra-chave para retornar uma única “página” de resultados, o Elasticsearch aceitaos parâmetros frome size:

Se você quiser mostrar cinco resultados por página, as páginas 1 a 3 podem ser solicitadas da seguinte forma:

GET /_search?size=5
GET /_search?size=5&from=5
GET /_search?size=5&from=10


Cuidado com a paginação muito profunda ou solicitando muitos resultados de uma só vez. Os resultados são classificados antes de serem retornados.
Mas lembre-se de que uma solicitação de pesquisa geralmente abrange vários fragmentos.
Cada fragmento gera seus próprios resultados classificados, que precisam ser classificados centralmente para garantir que a ordem geral esteja correta.

DEEP PAGING EM SISTEMAS DISTRIBUÍDOS
Para entender porque a paginação profunda é problemática, vamos imaginar que estamos pesquisando em um único índice com cinco fragmentos primários.
Quando solicitamos a primeira página de resultados (resultados de 1 a 10), cada fragmento produz seus próprios 10 principais resultados e os retorna ao nó solicitante ,
que classifica todos os 50 resultados para selecionar os 10 principais gerais.

Agora imagine que pedimos a página 1.000 – resultados de 10.001 a 10.010. Tudo funciona da mesma maneira, exceto que cada fragmento deve produzir seus 10.010 principais resultados.
O nó solicitante então classifica todos os 50.050 resultados e descarta 50.040 deles!

Você pode ver que, em um sistema distribuído, o custo da classificação dos resultados cresce exponencialmente quanto mais aprofundamos a paginação.
Há uma boa razão para que os mecanismos de pesquisa na web não retornem mais de 1.000 resultados para qualquer consulta.


---------------Pesquisa Lite---------------
Quando você indexa um documento, o Elasticsearch pega os valores de string de todos os seus campos e os concatena em uma grande string,
que ele indexa como o _allcampo especial.Por exemplo, quando indexamos este documento:

{
    "tweet":    "However did I manage before Elasticsearch?",
    "date":     "2014-09-14",
    "name":     "Mary Jones",
    "user_id":  1
}
é como se tivéssemos adicionado um campo extra chamado _allcom este valor:

"However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1"
A pesquisa de string de consulta usa o _allcampo, a menos que outro nome de campo tenha sido especificado.

---------Consultas mais complicadas-------------


A próxima consulta procura tweets, usando os seguintes critérios:

O namecampo contém mary ou john

O date é maior que 2014-09-10

O _allcampo contém uma das palavras aggregations ou geo

GET http://localhost:9200/_search?q=%2Bname%3A(mary+john)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo)

Como você pode ver nos exemplos anteriores, essa pesquisa de string de consulta lite é surpreendentemente poderosa.Sua sintaxe de consulta,
que é explicada em detalhes nos documentos de referência de sintaxe de string de consulta ,
nos permite expressar consultas bastante complexas de forma sucinta. Isso o torna ótimo para consultas descartáveis da linha de comando ou durante o desenvolvimento.

Por fim, a pesquisa de string de consulta permite que qualquer usuário execute consultas potencialmente lentas e pesadas em qualquer campo em seu índice,
possivelmente expondo informações privadas ou até mesmo deixando seu cluster de joelhos!

DICA
Por esses motivos, não recomendamos a exposição de pesquisas de string de consulta diretamente a seus usuários, a menos que sejam usuários avançados aos quais possa confiar seus dados e seu cluster.


-------------Capítulo 6. Mapeamento e Análise----------------


Enquanto brincamos com os dados em nosso índice, notamos algo estranho. Algo parece estar quebrado: temos 12 tweets em nossos índices, e apenas um deles contém a data 2014-09-15, mas dê uma olhada nos totalhits para as seguintes consultas:

GET /_search?q=2014              # 12 results
GET /_search?q=2014-09-15        # 12 results !
GET /_search?q=date:2014-09-15   # 1  result
GET /_search?q=date:2014         # 0  results !


O Elasticsearch gerou um mapeamento dinamicamente para nós, com base no que ele poderia adivinhar sobre nossos tipos de campo.
A resposta nos mostra que o datecampo foi reconhecido como um campo do tipo date.
O _allcampo não é mencionado porque é um campo padrão, mas sabemos que o _allcampo é do tipo string.

Mas, de longe, a maior diferença é entre os camposque representam valores exatos (que podem incluir stringcampos) e campos que representam texto completo.
Essa distinção é realmente importante - é o que separa um mecanismo de pesquisa de todos os outros bancos de dados.

-------------Valores exatos versus texto completo----------------

Os dados no Elasticsearch podem ser divididos em dois tipos: valores exatos e texto completo.

Os valores exatos são exatamente o que parecem. Os exemplos são uma data ou um ID de usuário,
mas também podem incluir strings exatas, como um nome de usuário ou um endereço de e-mail.
O valor exato "Foo" não é o mesmo que o valor exato "foo". O valor exato 2014 não é o mesmo que o valor exato 2014-09-15.

O texto completo, por outro lado, refere-sea dados textuais – geralmente escritos em alguma linguagem humana – como o texto de um tweet ou o corpo de um e-mail.


Os valores exatos são fáceis de consultar. A decisão é binária; um valor corresponde à consulta ou não. Esse tipo de consulta é fácil de expressar com SQL:

WHERE name    = "John Smith"
  AND user_id = 2
  AND date    > "2014-09-15"

Consultar dados de texto completo é muito mais sutil.
Não estamos apenas perguntando: “Este documento corresponde à consulta”, mas “Quão bem este documento corresponde à consulta?”
Em outras palavras, quão relevante é este documento para a consulta dada?

Raramente queremos corresponder exatamente todo o campo de texto completo. Em vez disso, queremos pesquisar nos campos de texto.
Não apenas isso, mas esperamos que a pesquisa entenda nossa intenção :


Uma pesquisa por UK também deve retornar documentos que mencionem o United Kingdom.

Uma pesquisa por jump também deve corresponder a jumped, jumps, jumpinge talvez até leap.

johnny walker deve corresponder Johnnie Walker, e johnnie depp deve corresponder Johnny Depp.

Fox news hunting deve retornar notícias sobre caça na Fox News, enquanto fox hunting news deve retornar notícias sobre caça à raposa.

Para facilitar esses tipos de consultas em campos de texto completo, o Elasticsearch primeiro analisa o texto e, em seguida, usa os resultados para criar um índice invertido

------------Capítulo 6. Mapeamento e Análise------------

Enquanto brincamos com os dados em nosso índice, notamos algo estranho.
Algo parece estar quebrado: temos 12 tweets em nossos índices, e apenas um deles contém a data 2014-09-15, mas dê uma olhada nos total hits para as seguintes consultas:

GET /_search?q=2014              # 12 results
GET /_search?q=2014-09-15        # 12 results !
GET /_search?q=date:2014-09-15   # 1  result
GET /_search?q=date:2014         # 0  results !

Mas, de longe, a maior diferença é entre os campos que representam valores exatos (que podem incluir string campos)
e campos que representam texto completo.
Essa distinção é realmente importante - é o que separa um mecanismo de pesquisa de todos os outros bancos de dados.

--------------Valores exatos versus texto completo-------------------------

Os dados no Elasticsearch podem ser divididos em dois tipos: valores exatos e texto completo.

Os valores exatos são exatamente o que parecem. Os exemplos são uma data ou um ID de usuário, mas também podem incluir strings exatas, como um nome de usuário ou um
endereço de e-mail. O valor exato Foonão é o mesmo que o valor exato foo. O valor exato 2014não é o mesmo que o valor exato 2014-09-15.

O texto completo, por outro lado, refere-sea dados textuais – geralmente escritos em alguma linguagem humana – como o texto de um tweet ou o corpo de um e-mail.

O texto completo é muitas vezes referido como dados não estruturados, que é um nome impróprio —
a linguagem natural é altamente estruturada. O problema é que as regras das linguagens naturais são complexas, o que as torna difíceis para os computadores analisarem corretamente.
Por exemplo, considere esta frase:

"Maio é divertido, mas junho me aborrece."

Refere-se a meses ou a pessoas?

Os valores exatos são fáceis de consultar. A decisão é binária; um valor corresponde à consulta ou não. Esse tipo de consulta é fácil de expressar com SQL:

WHERE name    = "John Smith"
  AND user_id = 2
  AND date    > "2014-09-15"


Consultar dados de texto completo é muito mais sutil.
Não estamos apenas perguntando: “Este documento corresponde à consulta”, mas “Quão bem este documento corresponde à consulta?”
Em outras palavras, quão relevante é este documento para a consulta dada?

Raramente queremos corresponder exatamente todo o campo de texto completo. Em vez disso, queremos pesquisar nos campos de texto. Não apenas isso, mas esperamos que a pesquisa entenda nossa intenção:

Uma pesquisa por UK também deve retornar documentos que mencionem o United Kingdom.

Uma pesquisa por jump também deve corresponder a jumped, jumps, jumpinge talvez até leap.

johnny walkerdeve corresponder Johnnie Walker, e johnnie depp deve corresponder Johnny Depp.

fox news hunting deve retornar notícias sobre caça na Fox News, enquanto fox hunting news deve retornar notícias sobre caça à raposa.

Para facilitar esses tipos de consultas em campos de texto completo, o Elasticsearch primeiro analisa o texto e, em seguida, usa os resultados para criar um índice invertido


---------------Índice invertido----------------------

O Elasticsearch usa uma estrutura chamadaum índice invertido , projetado para permitir pesquisas de texto completo muito rápidas.
Um índice invertido consiste em uma lista de todas as palavras únicas que aparecem em qualquer documento e, para cada palavra, uma lista dos documentos em que ela aparece.

Por exemplo, digamos que temos dois documentos, cada um com um contentcampo contendo o seguinte:

- A ligeira raposa marrom saltou sobre o cão preguiçoso

- Raposas marrons rápidas saltam sobre cães preguiçosos no verão

Content campo de cada documento emwords (que chamamos de termos , ou tokens ), crie uma lista ordenada de todos os termos exclusivos e, em seguida, liste em qual documento cada termo aparece.

Termo Doc_1 Doc_2
-------------------------
Rápido | | X
O | X |
marrom | X | X
cão | X |
cães | | X
raposa | X |
raposas | | X
em | | X
saltou | X |
preguiçoso | X | X
salto | | X
sobre | X | X
rápido | X |
verão | | X
o | X |
------------------------

Agora, se quisermos pesquisar por Rápido marrom, basta encontrar os documentos em que cada termo aparece:

Termo Doc_1 Doc_2
-------------------------
marrom | X | X
rápido | X |
------------------------
Total | 2 | 1

Ambos os documentos correspondem, mas o primeiro documento tem mais correspondências do que o segundo.
Se aplicarmos um algoritmo de similaridade ingênuo que apenas conta o número de termos correspondentes,
então podemos dizer que o primeiro documento é uma correspondência melhor - é mais relevante para nossa consulta - do que o segundo documento.

Mas existem alguns problemas com nosso índice invertido atual:

Rápido rápido aparecem como termos separados, enquanto o usuário provavelmente pensa neles como a mesma palavra.

raposa e raposas são bastante semelhantes, assim como cão e cães;Eles compartilham a mesma palavra raiz.

saltou e pulo, embora não sejam da mesma palavra raiz, são semelhantes em significado. São sinônimos.

Se normalizarmos os termos em um padrãoformato, podemos encontrar documentos que contêm termos que não são exatamente os mesmos que o usuário solicitou,
mas são semelhantes o suficiente para ainda serem relevantes. Por exemplo:

"Rápido" pode ser minúsculo para se tornar "rápido".

raposas espode ser desmembrado --reduzido à sua forma raiz- para se tornar raposa. Da mesma forma, cachoros poderia ser derivado de cachoro.

salto e pulo são sinônimos e podem ser indexados como um único termo salto.


Termo Doc_1 Doc_2
-------------------------
marrom | X | X
cão | X | X
raposa | X | X
em | | X
salto | X | X
preguiçoso | X | X
sobre | X | X
rápido | X | X
verão | | X
o | X | X
------------------------

Mas ainda não chegamos lá. Nossa busca por ainda+ rapidos +raposas falharia, porque não temos mais o termo exato em nosso índice.
No entanto, se aplicarmos as mesmas regras de normalização que usamos no campo para nossa string de consulta, ela se tornaria uma consulta para , que corresponderia a ambos os documentos!


-------------------------------Tipos de campo simples principais------------------------------
Fragmento:string

Número inteiro: byte, short, integer,long

Ponto flutuante: float,double

Boleano:boolean

Encontro:date


-------------------------------Personalizando mapeamentos de campo------------------------------

Enquanto os tipos de dados de campo básicos sãosuficiente para muitos casos,
muitas vezes você precisará personalizar o mapeamentopara campos individuais,
especialmente campos de string. Os mapeamentos personalizados permitem que você faça o seguinte:

Distinguir entre campos de string de texto completo e campos de string de valor exato

Use analisadores específicos de idioma

Otimizar um campo para correspondência parcial

Especificar formatos de data personalizados


O atributo mais importante de um campo é o type.
Para campos que não sejam stringcampos, você raramente precisará mapear algo além de type:

Os campos de tipo stringsão, por padrão, considerados como contendo texto completo.
Ou seja, seu valor será repassadoum analisador antes de ser indexado
e uma consulta de texto completo no campo passará a string de consulta por um analisador antes de pesquisar.

O valor padrão de indexpara um stringcampo é analyzed. Se quisermos mapear o campo como um valor exato, precisamos defini-lo como not_analyzed:

{
    "tag": {
        "type":     "string",
        "index":    "not_analyzed"
    }
}

Os outros tipos simples (como long, double, dateetc) também aceitam o indexparâmetro,
mas os únicos valores relevantes são noe not_analyzed, pois seus valores nunca são analisados.


------------------analisador--------------------


Para analyzedcampos de string, useo analyzeratributo para especificar qual analisador aplicar no momento da pesquisa e no momento do índice.
Por padrão, o Elasticsearch usa o standardanalisador, mas você pode alterar isso especificando um dos analisadores integrados, como whitespace, simple ou english

----------------Atualizando um mapeamento--------------------

Você pode especificar o mapeamento para um tipo quando você criar um índice.
Como alternativa, você pode adicionar o mapeamento para um novo tipo (ou atualizar o mapeamento para um tipo existente) posteriormente,
usando o ponto de /_mapping extremidade.

Embora você possa adicionar a um mapeamento existente, não pode alterá -lo. Se já existir um campo no mapeamento, os dados desse campo provavelmente já foram indexados. ]
Se você alterar o mapeamento de campo, os dados já indexados estarão errados e não poderão ser pesquisados adequadamente.

------------------------Tipos de campos principais complexos-----------------

Além dos tipos de dados escalares simples que mencionamos,
JSON também tem null valores, matrizes e objetos, todos suportados pelo Elasticsearch.

-------------------Campos de vários valores----------------------------------

Obs:

O _sourcecampo que você recebe de volta contém exatamente o mesmo documento JSON que você indexou.

Matrizes:

Não há mapeamento especial necessário para matrizes.Qualquer campo pode conter zero, um ou mais valores,
da mesma forma que um campo de texto completo é analisado para produzir vários termos.

No entanto, os arrays são indexados — tornados pesquisáveis
como campos de vários valores, que não são ordenados.No momento da pesquisa, você não pode se referir
a “o primeiro elemento” ou “o último elemento”. Em vez disso, pense em uma matriz como um pacote de valores.

--------------Objetos de vários níveis----------------------------


O Elasticsearch detectará novos campos de objeto
dinamicamente e mapeie-os como type object, com cada campo interno listado em properties:

No mapeamento fica algo como:

{
  "gb": {
    "tweet": { 1
      "properties": {
        "tweet":            { "type": "string" },
        "user": { 2
          "type":             "object",
          "properties": {
            "id":           { "type": "string" },
            "gender":       { "type": "string" },
            "age":          { "type": "long"   },
            "name":   { 2
              "type":         "object",
              "properties": {
                "full":     { "type": "string" },
                "first":    { "type": "string" },
                "last":     { "type": "string" }
              }
            }
          }
        }
      }
    }
  }
}

--------------Como os objetos internos são indexados------------

Lucene não entende objetos internos.Um documento Lucene consiste em uma lista simples de pares chave-valor.
Para que o Elasticsearch indexe objetos internos de maneira útil, ele converte nosso documento em algo assim:

{
    "tweet":            [elasticsearch, flexible, very],
    "user.id":          [@johnsmith],
    "user.gender":      [male],
    "user.age":         [26],
    "user.name.full":   [john, smith],
    "user.name.first":  [john],
    "user.name.last":   [smith]
}

No documento simplificado simples anterior, não há nenhum campo chamado user e nenhum campo chamado user.name.
O Lucene indexa apenas valores escalares ou simples, não estruturas de dados complexas.

-------------------------Matrizes de objetos internos---------------

Finalmente, considere como um array contendo objetos internos seriam indexados. Digamos que temos uma followers matriz que se parece com isso:

{
    "followers": [
        { "age": 35, "name": "Mary White"},
        { "age": 26, "name": "Alex Jones"},
        { "age": 19, "name": "Lisa Smith"}
    ]
}

Este documento será achatado como descrevemos anteriormente, mas o resultado ficará assim:

{
    "followers.age":    [19, 26, 35],
    "followers.name":   [alex, jones, lisa, smith, mary, white]
}

A correlação entre {age: 35}e {name: Mary White}foi perdida, pois cada campo de vários valores é apenas um pacote de valores, não uma matriz ordenada.
Isso é suficiente para perguntarmos: “Existe algum seguidor com 26 anos?”

Mas não podemos obter uma resposta precisa para isso: “Existe um seguidor que tem 26 anos e que se chama Alex Jones ?”


----------Capítulo 7. Pesquisa de corpo inteiro--------------------

Os autores do Elasticsearch preferem usar GET para uma solicitação de pesquisa porque acham que descreve a ação — recuperar informações — melhor do que o POST verbo.
No entanto, como GET um corpo de solicitação não é universalmente compatível, a searchAPI também aceita POST pedidos:

POST /_search
{
  "from": 30,
  "size": 10
}

----------------Consulta DSL-----------------------------------

A consulta DSL é uma pesquisa flexível e expressiva linguagem que o Elasticsearch usa para expor a maior parte do poder do Lucene por meio de uma interface JSON simples.
É o que você deve usar para escrever suas consultas em produção. Isso torna suas consultas mais flexíveis, mais precisas, mais fáceis de ler e mais fáceis de depurar.

Para usar o Query DSL, passe uma query no query parâmetro:

GET /_search
{
    "query": {
        "match_all": {}
    }
}

-------------------Estrutura de uma Cláusula de Consulta------------

Uma cláusula de consulta normalmentetem esta estrutura:

{
    QUERY_NAME: {
        ARGUMENT: VALUE,
        ARGUMENT: VALUE,...
    }
}

Se fizer referência a um campo específico, terá esta estrutura:

{
    QUERY_NAME: {
        FIELD_NAME: {
            ARGUMENT: VALUE,
            ARGUMENT: VALUE,...
        }
    }
}

A solicitação de pesquisa completa ficaria assim:

GET /_search
{
    "query": {
        "match": {
            "tweet": "elasticsearch"
        }
    }
}

--------------------Combinando várias cláusulas-----------------------------

Cláusulas de consulta são blocos de construção simplesque podem ser combinados entre si para criar consultas complexas. As cláusulas podem ser as seguintes:

Cláusulas folha (como a matchcláusula) quesão usados para comparar um campo (ou campos) com uma string de consulta.

Cláusulas compostas que são usadas para combinar outras cláusulas de consulta.
Por exemplo, uma bool cláusula permite combinar outras cláusulas que must correspondam, must_not correspondam ou should correspondam, se possível:

{
    "bool": {
        "must":     { "match": { "tweet": "elasticsearch" }},
        "must_not": { "match": { "name":  "mary" }},
        "should":   { "match": { "tweet": "full text" }}
    }
}

-------------------Consultas e filtros-------------------------------------

Embora nos refiramos à DSL de consulta, na realidade existem duas DSLs: a DSL de consulta e a DSL de filtro

Um filtro faz uma pergunta sim|não decada documento e é usado para campos que contêm valores exatos

Uma consulta é semelhante a um filtro, mas também solicita a pergunta: Quão bem este documento corresponde?

Uma consulta calcula a relevância de cada documentoé para a consulta e atribui a ela uma relevância _score,
que é usada posteriormente para classificar os documentos correspondentes por relevância.
Esse conceito de relevância é bem adequado para pesquisa de texto completo, onde raramente há uma resposta completamente “correta”.

--------------Diferenças de desempenho--------------------------------

A saída da maioria das cláusulas de filtro - um simpleslista dos documentos que correspondem ao filtro é rápido de calcular e fácil de armazenar em cache na memória,
usando apenas 1 bit por documento. Esses filtros em cache podem ser reutilizados com eficiência para solicitações subsequentes.

As consultas precisam não apenas encontrar documentos correspondentes, mas também calculam a relevância de cada documento,
o que normalmente torna as consultas mais pesadas do que os filtros. Além disso, os resultados da consulta não podem ser armazenados em cache.

Graças ao índice invertido, uma consulta simples que corresponde a apenas alguns documentos pode ter um desempenho tão bom ou melhor do que um filtro em cache que abrange milhões de documentos.
Em geral, no entanto, um filtro em cache superará uma consulta e o fará de forma consistente.

O objetivo dos filtros é reduzir o número de documentos que precisam ser examinados pela consulta.

Quando usar qual

Como regra geral, use cláusulas de consulta para pesquisa de texto completo ou para
qualquer condição que deva afetar a pontuação de relevância e use cláusulas de filtro para todo o resto.

--------------------Filtro de termo-------------------------------------

O term filtro é usado para filtrar porvalores exatos, sejam eles números, datas, booleanos ou not_analyzed campos de string de valor exato

{ "term": { "age":    26           }}
{ "term": { "date":   "2014-09-01" }}
{ "term": { "public": true         }}
{ "term": { "tag":    "full_text"  }}

---------------Filtro de intervalo-------------------------------------

O range filtro permite que você encontrenúmeros ou datas que se enquadram em um intervalo especificado:

{
    "range": {
        "age": {
            "gte":  20,
            "lt":   30
        }
    }
}

Os operadores que ele aceita são os seguintes:

-gt-
Maior que
-gte-
Maior que ou igual a
lt
Menor que
lte
Menos que ou igual a

-------------------existe e está faltando Filtros------------------


Os filtros existse sãomissingusado para localizar documentos nos quais o campo especificado possui um ou mais valores
( exists) ou não possui valores ( missing). É de natureza semelhante a IS_NULL(missing) e NOT IS_NULL(exists) em SQL:

{
    "exists":   {
        "field":    "title"
    }
}

Esses filtros são frequentemente usados aplicar uma condição somente se um campo estiver presente e para aplicar uma condição diferente se estiver ausente.

-------------------------filtro booleano----------------------------------

O bool filtro é usado para combinar várias cláusulas de filtro usando lógica booleana. Ele aceita três parâmetros:

must
Essas cláusulas devem corresponder, como and.

must_not
Essas cláusulas não devem corresponder, como not.

should
Pelo menos uma dessas cláusulas deve corresponder, como or.

Cada um desses parâmetros pode aceitar uma única cláusula de filtro ou uma matriz de cláusulas de filtro:

{
    "bool": {
        "must":     { "term": { "folder": "inbox" }},
        "must_not": { "term": { "tag":    "spam"  }},
        "should": [
                    { "term": { "starred": true   }},
                    { "term": { "unread":  true   }}
        ]
    }
}

-------------------match_all Consulta--------------------------------------

A match_allconsulta simplesmente corresponde a todos os documentos. É a consulta padrão usada se nenhuma consulta tiver sido especificada:

{ "match_all": {}}

Essa consulta é frequentemente usada em combinação com um filtro, por exemplo, para recuperar todos os emails na pasta da caixa de entrada.
Todos os documentos são considerados igualmente relevantes, portanto, todos recebem um neutro _score de 1.

-----------------Consulta de correspondência-------------------------------

A match consulta deve ser o padrão consulta que você acessa sempre que deseja consultar um valor de texto completo ou exato em praticamente qualquer campo.

Se você executar uma match consulta em um campo de texto completo, ele analisará a string de consulta usando o analisador correto para esse campo antes de executar a pesquisa:

{ "match": { "tweet": "About Search" }}

Se você usá-lo em um campo contendo um valor exato,como um número, uma data, um booleano ou um not_analyzed campo de string, ele procurará esse valor exato:

{ "match": { "age":    26           }}
{ "match": { "date":   "2014-09-01" }}
{ "match": { "public": true         }}
{ "match": { "tag":    "full_text"  }}


Para pesquisas de valor exato, você provavelmente deseja usar um filtro em vez de uma consulta, pois um filtro será armazenado em cache.

Ao contrário da pesquisa de string de consulta que mostramos em “Search Lite ” , a match consulta não usa uma sintaxe de consulta como +user_id:2 +tweet:search.
Ele apenas procura as palavras que são especificadas.

----------------------------Consulta multi_match--------------------------------

A multi_match consulta permitepara executar a mesma matchconsulta em vários campos:

{
    "multi_match": {
        "query":    "full text search",
        "fields":   [ "title", "body" ]
    }
}


--------------------------bool Consulta------------------------------------------

A bool consulta, como o boolfiltro, é usado para combinar várias cláusulas de consulta. No entanto, existem algumas diferenças.
Lembre-se de que, enquanto os filtros fornecem respostas binárias de sim/não, as consultas calculam uma pontuação de relevância.

A bool consulta combina a cláusula _score from each must ou correspondente. should Esta consulta aceita os seguintes parâmetros:

must
Cláusulas que devem corresponder para que o documento seja incluído.

must_not
Cláusulas que não devem corresponder para que o documento seja incluído.

should
Se essas cláusulas corresponderem, elas aumentarão o _score; caso contrário, eles não têm efeito. Eles são usados simplesmente para refinar
a pontuação de relevância para cada documento.

A consulta a seguir encontra documentos cujo titl e campo corresponde à string de consulta how to make millions e que não estão marcados como spam.

Documentos que correspondam a ambas as condições terão uma classificação ainda mais alta:

{
    "bool": {
        "must":     { "match": { "title": "how to make millions" }},
        "must_not": { "match": { "tag":   "spam" }},
        "should": [
            { "match": { "tag": "starred" }},
            { "range": { "date": { "gte": "2014-01-01" }}}
        ]
    }
}

Se não houver must cláusulas, pelo menos uma should cláusula deve corresponder.
No entanto, se houver pelo menos uma must cláusula, nenhuma should cláusula será necessária para corresponder.


-------------------------Filter---------------------------------------------------

filter

A cláusula (consulta) deve aparecer nos documentos correspondentes. No entanto ao contrário must da pontuação da consulta será ignorada.
As cláusulas de filtro são executadas no contexto de filtro, o que significa que a pontuação é ignorada e as cláusulas são consideradas para armazenamento em cache.

Exemplo:


{
  "query": {
    "bool": {
      "should": [
        {"term" : { "name" : "Mary" }}
      ],
      "filter": [
        {"term": {
          "user_id": 2
        }},
        {"term": {
          "date": "2014-09-15"
        }}
      ]
    }
  }
}

https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-bool-query.html

Ou seja o filter é tudo que bate com aquilo, utilizado pra data equivalentes, tipos, estados, coisas exatas e datas.
Exemplo com data:

/tweet/_validate/query?explain --< utilizado para validar

{
  "query": {
    "bool": {
      "filter": [
        {"term": {
          "user_id": 2
        }},
        {"range" : {
          "date" : { "gte" : "2011-09-23", "lte" : "2020-09-23" }
        }}
      ]
    }
  }
}


----------------------Uma consulta como filtro-----------------------------

Ocasionalmente, você desejará usar uma consulta enquanto estiver no contexto do filtro.
Isto pode ser conseguido com o query filtro, queapenas envolve uma consulta.
O exemplo a seguir mostra uma maneira de excluir e-mails que parecem spam:

GET /_search
{
    "query": {
        "filtered": {
            "filter":   {
                "bool": {
                    "must":     { "term":  { "folder": "inbox" }},
                    "must_not": {
                        "query": { 1
                            "match": { "email": "urgent business proposal" }
                        }
                    }
                }
            }
        }
    }
}

------------------------Validando consultas-------------------------------

As consultas podem se tornar bastante complexas e, especialmente quando combinado com diferentes analisadores e mapeamentos de campo,
pode se tornar um pouco difícil de seguir.
A validate-query API pode ser usada para verificar se uma consulta é válida.

GET /gb/tweet/_validate/query
{
   "query": {
      "tweet" : {
         "match" : "really powerful"
      }
   }
}

----------------------Noções básicas sobre erros--------------------------

Descobrir por que é inválido, adicione o explain parâmetropara a string de consulta:

GET /gb/tweet/_validate/query?explain
{
   "query": {
      "tweet" : {
         "match" : "really powerful"
      }
   }
}


O explain sinalizador fornece mais informações sobre por que uma consulta é inválida.

Aparentemente, confundimos o tipo de consulta ( match) com o nome do campo ( tweet):

{
  "valid" :     false,
  "_shards" :   { ... },
  "explanations" : [ {
    "index" :   "gb",
    "valid" :   false,
    "error" :   "org.elasticsearch.index.query.QueryParsingException:
                 [gb] No query registered for [tweet]"
  } ]
}


------------------Noções básicas sobre consultas--------------------------------

O uso do explain parâmetro tem a vantagem adicional de retornar uma descrição legível por humanos da consulta (válida),
o que pode ser útil para entender exatamente como sua consulta foi interpretada pelo Elasticsearch:

GET /_validate/query?explain
{
   "query": {
      "match" : {
         "tweet" : "really powerful"
      }
   }
}

{
  "valid" :         true,
  "_shards" :       { ... },
  "explanations" : [ {
    "index" :       "us",
    "valid" :       true,
    "explanation" : "tweet:really tweet:powerful"
  }, {
    "index" :       "gb",
    "valid" :       true,
    "explanation" : "tweet:realli tweet:power"
  } ]
}

No explanation, você pode ver como a match consulta da string de consulta really powerful
foi reescrita como duas consultas de termo único no tweet campo, uma para cada termo.

----------------------Capítulo 8. Classificação e Relevância-------------------


Por padrão, os resultados são retornados classificados por relevância — com os documentos mais relevantes primeiro.
Mais adiante neste capítulo, explicaremos o que queremos dizer com relevância e como ela é calculada,
mas vamos começar examinando o sort parâmetro e como usá-lo.

----------------------Ordenação----------------------------------------------

Para classificar por relevância, precisamos representar a relevância como um valor. No Elasticsearch,
a pontuação de relevância é representada pelo número de ponto flutuante retornado nos resultados da pesquisa como _score,
portanto, a ordem de classificação padrão é _score decrescente.

GET /_search
{
    "query" : {
        "filtered" : {
            "filter" : {
                "term" : {
                    "user_id" : 1
                }
            }
        }
    }
}

Os filtros não influenciam _score, e a consulta ausente mas implícita match_all apenas define o _score valor neutro de 1 para todos os documentos.


------------------------Classificando por valores de campo------------------------

Nesse caso, provavelmente faz sentido classificar os tweets por recência, com os tweets mais recentes primeiro. Podemos fazer isso com o sort parâmetro:

GET /_search
{
    "query" : {
        "filtered" : {
            "filter" : { "term" : { "user_id" : 1 }}
        }
    },
    "sort": { "date": { "order": "desc" }}
}

Saída:

"hits" : {
    "total" :           6,
    "max_score" :       null, 1
    "hits" : [ {
        "_index" :      "us",
        "_type" :       "tweet",
        "_id" :         "14",
        "_score" :      null, 1
        "_source" :     {
             "date":    "2014-09-24",
             ...
        },
        "sort" :        [ 1411516800000 ] 2
    },
    ...
}

O _score não é calculado porque não está sendo usado para ordenação.

O valor do date campo, expresso em milissegundos desde a época, é retornado nos sort valores.

A primeira é que temos um novo elemento em cada resultado chamado sort, que contém os valores que foram usados para classificação.
Neste caso, classificamos em date, que internamente é indexado como milissegundos desde a época.
O número longo 1411516800000 é equivalente à string de data 2014-09-24 00:00:00 UTC.

A segunda é que o _score e max_score são ambos null. Calcular o _score pode ser bastante caro, e normalmente seu único propósito é ordenar.
Se você quiser que o _score cálculo seja feito de qualquer maneira, você pode definir o track_scores parâmetro para true.

Como atalho, você pode especifique apenas o nome do campo para classificar:

    "sort": "number_of_children"

Os campos serão ordenados em ordem crescente por padrão e o _scorevalor em ordem decrescente.

--------------------Classificação multinível-----------------------

Talvez queiramos combinar o _score de uma query com date, e mostre todos os resultados correspondentes classificados primeiro por data e depois
por relevância(score_):

GET /_search
{
    "query" : {
        "filtered" : {
            "query":   { "match": { "tweet": "manage text search" }},
            "filter" : { "term" : { "user_id" : 2 }}
        }
    },
    "sort": [
        { "date":   { "order": "desc" }},
        { "_score": { "order": "desc" }}
    ]
}

A ordem é importante. Os resultados são classificados primeiro pelo primeiro critério.
Somente os resultados cujo primeiro sortvalor é idêntico serão classificados pelo segundo critério e assim por diante.

--------------------Classificando em campos de vários valores-------------------

Ao classificar em campos com mais de um valor, lembre-se que os valores não possuem nenhuma ordem intrínseca;
um campo multivalor é apenas um pacote de valores. Em qual você escolhe classificar?

Para números e datas, você pode reduzir um campo de vários valores a um único valor usando os mode min , max, avgou de classificação .sum
Por exemplo, você pode classificar pela data mais antiga em cada datescampo usando o seguinte:

"sort": {
    "dates": {
        "order": "asc",
        "mode":  "min"
    }
}

-----------------------Classificação de Strings e Multicampos---------------------

Os campos de string analisados também são campos de vários valores, mas classificá-los raramente fornece os resultados desejados.
Se você analisar uma string como fine old art, resultará em três termos.
Provavelmente queremos classificar em ordem alfabética no primeiro termo, depois no segundo termo e assim por diante, mas o Elasticsearch não tem essas informações à disposição no momento da classificação.


Para classificar em um campo de string, esse campo deve conter apenas um termo: a not_analyzed string inteira.
Mas é claro que ainda precisamos que o campo esteja analyzedpara poder consultá-lo como texto completo.

A abordagem ingênua para indexar a mesma string de duas maneiras seria incluir dois campos separados no documento:
um analyzed para pesquisa e outro not_analyzed para classificação.

Mas armazenar a mesma string duas vezes no _source campo é desperdício de espaço.
O que realmente queremos fazer é passar em um único campo, mas indexá-lo de duas maneiras diferentes.
Todos os tipos de campos principais (strings, números, booleanos, datas) aceitam um fields parâmetro que permite transformar um mapeamento simples como:

"tweet": {
    "type":     "string",
    "analyzer": "english"
}

em um mapeamento de vários campos como este:

"tweet": { 1
    "type":     "string",
    "analyzer": "english",
    "fields": {
        "raw": { 2
            "type":  "string",
            "index": "not_analyzed"
        }
    }
}

1: O tweet campo principal é exatamente o mesmo de antes: um analyzedcampo de texto completo.

2: O novo tweet.raw subcampo é not_analyzed.

Agora, ou pelo menos assim que reindexamos nossos dados, podemos usar o tweet campo para pesquisa e o tweet.rawcampo para classificação:

GET /_search
{
    "query": {
        "match": {
            "tweet": "elasticsearch"
        }
    },
    "sort": "tweet.raw"
}

A classificação em um campo de texto completo analyzed pode usar muita memória. Consulte “Dados de campo” para obter mais informações.

-----------------------Mudança de tipo String para text e keyword-------------------

Para evitar esses problemas, o stringcampo foi dividido em dois novos tipos: text,
que deve ser usado para pesquisa de texto completo, e keyword, que deve ser usado para pesquisa por palavra-chave.

Como consequência, será possível realizar pesquisa de texto completo em foo e pesquisa de palavras-chave e agregações usando o foo.keywordcampo.


------------------------O que é relevância?---------------------------

A pontuação de relevância de cada documento é representada por um número de ponto flutuante positivo chamado _score.
Quanto maior o _score, mais relevante o documento.

Uma cláusula de consulta gera um _scorepara cada documento. Como essa pontuação é calculada depende do tipo de cláusula de consulta.
Cláusulas de consulta diferentes são usadas para propósitos diferentes: uma fuzzy consulta pode determinar o _score cálculo da similaridade da ortografia da palavra encontrada com o termo de pesquisa original;
uma terms consulta incorporaria a porcentagem de termos encontrados.
No entanto, o que geralmente queremos dizer com relevância é o algoritmo que usamos para calcular a semelhança entre
o conteúdo de um campo de texto completo e uma string de consulta de texto completo.

O algoritmo de similaridade padrão usado no Elasticsearch é conhecido como frequência de termo/frequência de documento inversa,
ou TF/IDF , que leva em consideração os seguintes fatores em conta:


Frequência do prazo
Com que frequência o termo aparece no campo? Quanto mais vezes, mais relevante.
Um campo contendo cinco menções do mesmo termo tem maior probabilidade de ser relevante do que um campo contendo apenas uma menção.

Frequência inversa do documento
Com que frequência cada termo aparece no índice? Quanto mais frequente, menos relevante.
Os termos que aparecem em muitos documentos têm um peso menor do que os termos mais incomuns.

Norma de comprimento de campo
Qual o comprimento do campo? Quanto mais longo, menos provável é que as palavras no campo sejam relevantes.
Um termo que aparece em um title campo curto tem mais peso do que o mesmo termo que aparece em um content campo longo.

Individualas consultas podem combinar a pontuação TF/IDF com outros fatores, como o termo proximidade em consultas de frase ou semelhança de termo em consultas difusas.

A relevância não é apenas sobre a pesquisa de texto completo, no entanto.
Também pode ser aplicado a cláusulas sim/não, onde quanto mais cláusulas corresponderem, maior será o _score.

Quando várias cláusulas de consulta são combinadas usando uma consulta composta como a bool consulta,
a _score de cada uma dessas cláusulas de consulta é combinada para calcular o total _score do documento.

----------------------Entendendo a pontuação--------------------------------------

Ao depurar uma consulta complexa, pode ser difícil entender exatamente como a _score foi calculado.
O Elasticsearch tem a opção de produzir uma explicação com cada resultado da pesquisa, definindo o explain parâmetropara true.

GET /_search?explain
{
   "query"   : { "match" : { "tweet" : "honeymoon" }}
}


O explain parâmetro adiciona uma explicação de como _score foi calculado a cada resultado.

Primeiro, temos os metadados que são retornados em solicitações de pesquisa normais:

{
    "_index" :      "us",
    "_type" :       "tweet",
    "_id" :         "12",
    "_score" :      0.076713204,
    "_source" :     { ... trimmed ... },


Ele adiciona informações sobre o estilhaço e o nó de origem do documento,
o que é útil saber porque as frequências de termo e documento são calculadas por estilhaço, e não por índice:

   "_shard" :      1,
    "_node" :       "mzIVYCsqSWCG_M_ZffSs9Q",


Em seguida, fornece o _explanation. Cada A entrada contém um description que informa que tipo de cálculo está sendo executado,
um value que fornece o resultado do cálculo e o detailsde quaisquer subcálculos necessários:

"_explanation": { 1
   "description": "weight(tweet:honeymoon in 0)
                  [PerFieldSimilarity], result of:",
   "value":       0.076713204,
   "details": [
      {
         "description": "fieldWeight in 0, product of:",
         "value":       0.076713204,
         "details": [
            {  2
               "description": "tf(freq=1.0), with freq of:",
               "value":       1,
               "details": [
                  {
                     "description": "termFreq=1.0",
                     "value":       1
                  }
               ]
            },
            { 3
               "description": "idf(docFreq=1, maxDocs=1)",
               "value":       0.30685282
            },
            { 4
               "description": "fieldNorm(doc=0)",
               "value":        0.25,
            }
         ]
      }
   ]
}

1 - Resumo do cálculo de pontuação parahoneymoon

2 - Frequência do prazo

3 - Frequência inversa do documento

4 - Norma de comprimento de campo

A primeira parte é o resumo do cálculo.
Diz-nos que calculou o peso — oTF/IDF—do termo honeymoonno campo tweet, para documento 0.
(Este é um ID de documento interno e, para nossos propósitos, pode ser ignorado.)

Frequência do prazo
Quantas vezes o termo honeymoon apareceu no tweetcampo neste documento?

Frequência inversa do documento
Quantas vezes o termo honeymoon apareceu no tweetcampo de todos os documentos do índice?

Norma de comprimento de campo
Qual é o comprimento do tweetcampo neste documento? Quanto maior o campo, menor esse número.


As explicações para consultas mais complicadas podem parecer muito complexas, mas na verdade elas contêm apenas mais dos mesmos cálculos que aparecem no exemplo anterior.
Essas informações podem ser inestimáveis para depurar por que os resultados da pesquisa aparecem na ordem em que aparecem.

DICA
A saída explain pode ser difícil de ler em JSON, mas é mais fácil quando formatada como YAML. Basta adicionar format=yaml à string de consulta.



----------------------Entendendo por que um documento corresponde-----------------------------


Embora a explain opção adicione uma explicação para cada resultado, você pode usar a explain API para entender por que um
determinado documento correspondeu ou, mais importante, por que não correspondeu.

O caminho para a solicitação é /index/type/id/_explain, como no seguinte:

GET /us/tweet/12/_explain
{
   "query" : {
      "filtered" : {
         "filter" : { "term" :  { "user_id" : 2           }},
         "query" :  { "match" : { "tweet" :   "honeymoon" }}
      }
   }
}

------------------Dados de campo------------------------------------------------------------

Quando você classifica em um campo, o Elasticsearch precisa acessar o valor desse campo para cada documento que corresponda à consulta.
O índice invertido, que funciona muito bem na busca, não é a estrutura ideal para ordenar os valores dos campos:

* Ao pesquisar, precisamos mapear um termo para uma lista de documentos.

* Ao classificar, precisamos mapear um documento para seus termos. Em outras palavras, precisamos “desinverter” o índice invertido.

Para tornar a classificação eficiente, o Elasticsearch carrega todos os valores do campo que você deseja classificar na memória.
Isso é conhecido como fielddata.

O Elasticsearch não carrega apenas os valores dos documentos que correspondem a uma consulta específica.
Ele carrega os valores de cada documento em seu índice , independentemente do documento type.


O motivo pelo qual o Elasticsearch carrega todos os valores na memória é que a desinversão do índice do disco é lenta.
Mesmo que você precise dos valores de apenas alguns documentos para a solicitação atual, provavelmente precisará acessar os valores de outros documentos na próxima solicitação,
portanto, faz sentido carregar todos os valores na memória de uma só vez e manter eles lá.

Fielddata é usado em vários lugares no Elasticsearch:

- Classificando em um campo

- Agregações em um campo

- Certos filtros (por exemplo, filtros de geolocalização)

- Scripts que se referem a campos

Claramente, isso pode consumir muita memória, especialmente para campos de string de alta cardinalidade — campos de string que têm muitos valores exclusivos — como o corpo de um email.
Felizmente, memória insuficiente é um problema que pode ser resolvido pelo dimensionamento horizontal, adicionando mais nós ao cluster.

Dados de campo e estar ciente de que eles podem consumir muita memória.

------------------------Capítulo 9. Execução de Pesquisa Distribuída------------------------

A pesquisa requer um modelo de execução mais complicado porque não sabemos quais documentos corresponderão à consulta: eles podem estar em qualquer fragmento do cluster.
Uma solicitação de pesquisa deve consultar uma cópia de cada fragmento no índice ou índices nos quais estamos interessados para ver se eles têm algum documento correspondente.

Mas encontrar todos os documentos correspondentes é apenas metade da história.
Os resultados de vários fragmentos devem ser combinados em uma única lista classificada antes que a search API possa retornar uma “página” de resultados.
Por esse motivo, a pesquisa é executada em um processo de duas fases chamado query then fetch.

------------------------Fase de consulta----------------------------------------------------

Durante a fase de consulta inicial , A consulta é transmitida para uma cópia de estilhaço (um estilhaço primário ou de réplica) de cada estilhaço no índice.
Cada fragmento executa a pesquisa localmente ecria uma fila de prioridade de documentos correspondentes.


Uma fila de prioridade é apenas uma lista ordenada que contém os n principais documentos correspondentes. O tamanho da fila de prioridade depende dos parâmetros de paginação frome size.
Por exemplo, a seguinte solicitação de pesquisa exigiria uma fila de prioridade grande o suficiente para conter 100 documentos:

GET /_search
{
    "from": 90,
    "size": 10
}

A fase de consulta consiste nas três etapas a seguir:

1 - O cliente envia uma search solicitação para Node 3, que cria uma fila de prioridade vazia de tamanho from + size.

2 - Node 3 encaminha a solicitação de pesquisa para uma cópia primária ou de réplica de cada estilhaço no índice.
Cada fragmento executa a consulta localmente e adiciona os resultados em uma fila de prioridade classificada local de tamanho from + size.

3 - Cada fragmento retorna os IDs de documentos e os valores de classificação de todos os documentos em sua fila de prioridade para o nó coordenador,
Node 3, que mescla esses valores em sua própria fila de prioridade para produzir uma lista de resultados classificada globalmente.

Quando uma solicitação de pesquisa é enviada a um nó, esse nó se torna o nó coordenador.
É o trabalho desse nó transmitir a solicitação de pesquisa para todos os shards envolvidos e reunir suas respostas em um conjunto de resultados classificado globalmente que ele pode retornar ao cliente.

A primeira etapa é transmitir a solicitação para uma cópia de fragmentação de cada nó no índice.
Assim como as solicitações de documentos , as GETsolicitações de pesquisa podem ser tratadas por um estilhaço primário ou por qualquer uma de suas réplicas.
É assim que mais réplicas (quando combinadas com mais hardware) podem aumentar o rendimento da pesquisa.

Um nó de coordenação fará o round-robin em todas as cópias de fragmentos em solicitações subsequentes para distribuir a carga.

O fragmento carrega os corpos do documento — o _source campo — e, se solicitado, enriquece os resultados com metadados e destaque de trecho de pesquisa .
Uma vez que o nó coordenador recebe todos os resultados, ele os reúne em uma única resposta que retorna ao cliente.


---------------Opções de pesquisa-------------------------


-----tipo de busca----

Enquanto query_then_fetch é o padrão tipo de pesquisa, outros tipos de pesquisa podem ser especificados para fins específicos, por exemplo:

GET /_search?search_type=count

count:

O count tipo de pesquisa tem apenas uma query fase.
Ele pode ser usado quando você não precisa de resultados de pesquisa, apenas uma contagem de documentos ou agregações em documentos que correspondam à consulta.

query_and_fetch:

O query_and_fetch tipo de pesquisa combina as fases de consulta e busca em uma única etapa.
Essa é uma otimização interna que é usada quando uma solicitação de pesquisa é direcionada apenas a um único estilhaço, como quando um routing valor foi especificado.
Embora você possa optar por usar esse tipo de pesquisa manualmente, quase nunca é útil fazê-lo.

dfs_query_then_fetchedfs_query_and_fetch:

Os dfs tipos de pesquisa tem uma fase de pré-consulta que busca as frequências de termos de todos os shards envolvidos para calcular as frequências de termos globais.

scan:

O scan tipo de pesquisa é usado em conjunto com a scroll API para recuperar um grande número de resultados com eficiência.
Ele faz isso desativando a classificação.



-------digitalizar e rolar-----------

scroll:

Uma pesquisa rolada nos permite faça uma pesquisa inicial e continue extraindo lotes de resultados do Elasticsearch até que não haja mais resultados.
É um pouco como um cursor emum banco de dados tradicional.

Uma pesquisa rolada tira um instantâneo no tempo.
Ele não vê nenhuma alteração feita no índice depois que a solicitação de pesquisa inicial foi feita.
Ele faz isso mantendo os arquivos de dados antigos por perto, para que possa preservar sua “visão” sobre a aparência do índice no momento em que foi iniciado.

scan:

A parte cara da paginação profunda é a classificação global dos resultados, mas se desabilitarmos a classificação, podemos retornar todos os documentos de forma bastante barata.
Para fazer isso, usamos o scantipo de pesquisa.
Scan instrui o Elasticsearch a não fazer nenhuma classificação, mas apenas retornar o próximo lote de resultados de cada fragmento que ainda tem resultados a serem retornados.

GET /old_index/_search?search_type=scan&scroll=1m 1-
{
    "query": { "match_all": {}},
    "size":  1000
}
*1 - Mantenha o pergaminho aberto por 1 minuto.

A resposta a esta solicitação não inclui nenhum hit, mas inclui um _scroll_id, que é um longo codificado em Base-64.
Agora podemos passar _scroll_id para o _search/scroll end-point para recuperar o primeiro lote de resultados:

GET /_search/scroll?scroll=1m 1
c2Nhbjs1OzExODpRNV9aY1VyUVM4U0NMd2pjWlJ3YWlBOzExOTpRNV9aY1VyUVM4U0 2
NMd2pjWlJ3YWlBOzExNjpRNV9aY1VyUVM4U0NMd2pjWlJ3YWlBOzExNzpRNV9aY1Vy
UVM4U0NMd2pjWlJ3YWlBOzEyMDpRNV9aY1VyUVM4U0NMd2pjWlJ3YWlBOzE7dG90YW
xfaGl0czoxOw==

1 - Mantenha o pergaminho aberto por mais um minuto.


2 - O _scroll_id pode ser passado no corpo, na URL ou como parâmetro de consulta.

Observe que especificamos novamente ?scroll=1m. O tempo de expiração da rolagem é atualizado toda vez que executamos uma solicitação de rolagem, portanto,
ele precisa nos dar apenas tempo suficiente para processar o lote atual de resultados, não todos os documentos que correspondem à consulta.

A resposta a essa solicitação de rolagem inclui o primeiro lote de resultados.
Embora tenhamos especificado um sizede 1.000, recebemos muitos outros documentos.
Ao digitalizar, o size é aplicado a cada fragmento, para que você receba de volta o máximo de size * number_of_primary_shards documentos em cada lote.

A solicitação de rolagem também retorna um novo arquivo _scroll_id.
Toda vez que fazemos a próxima solicitação de rolagem, devemos passar o _scroll_id retornado pela solicitação de rolagem anterior.

-------------------------------Capítulo 10. Gerenciamento de Índice-------------------------------------------------------

---------Criando um índice-------------
extra:

https://opster.com/analysis/elasticsearch-updated-breaker-settings-parent/

O índice é criado com as configurações padrão e novos campos são adicionados ao mapeamento de tipo usando o mapeamento dinâmico.

Para isso, temos que criar o índice manualmente, passando qualquer configuração ou mapeamento de tipo no corpo da requisição, como segue:

PUT /my_index
{
    "settings": { ... any settings ... },
    "mappings": {
        "type_one": { ... any mappings ... },
        "type_two": { ... any mappings ... },
        ...
    }
}

Na verdade, se você quiser, você pode impedir a criação automática de índices adicionando a seguinte configuração ao config/elasticsearch.yml arquivo em cada nó:

action.auto_create_index: false

----------------Como excluir um índice---------------------


Para excluir um índice, useo seguinte pedido:

DELETE /my_index

Você pode excluir vários índices com isso:

DELETE /index_one,index_two
DELETE /index_*

Você pode até excluir todos os índices com isso:

DELETE /_all

----------------------Como a Lucene vê os documentos---------------

Um documento no Lucene consiste em uma lista simples de pares de valor de campo.
Um campo deve ter pelo menos um valor, mas qualquer campo pode conter vários valores.
Da mesma forma, um único valor de string pode ser convertido em vários valores pelo processo de análise.
O Lucene não se importa se os valores são strings, números ou datas—todos os valores são tratados apenas como bytes opacos.

Quando indexamos um documento no Lucene, os valores de cada campo são adicionados ao índice invertido do campo associado.
Opcionalmente, os valores originais também podem ser armazenados inalterados para que possam ser recuperados posteriormente.

-----------------------Como os tipos são implementados-----------------

Um índice pode ter vários tipos, cada um com seu próprio mapeamento, e documentos de qualquer um desses tipos podem ser armazenados no mesmo índice.
Como o Lucene não tem nenhum conceito de tipos de documentos, o nome do tipo de cada documento é armazenado com o documento em um campo de meta dados chamado _type.

A Lucene também não tem noção de mapeamentos.
Os mapeamentos são a camada que o Elasticsearch usa para mapear documentos JSON complexos nos documentos simples e simples que o Lucene espera receber.


------------------Capítulo 11. Dentro de um fragmento--------------------

 É por isso que dizemos que o Elasticsearch tem pesquisa quase em tempo real: as alterações do documento não são visíveis para pesquisa imediatamente, mas ficarão visíveis em 1 segundo.


 Isso pode ser confuso para novos usuários: eles indexam um documento e tentam procurá-lo, mas ele simplesmente não está lá. A maneira de contornar isso é realizar uma atualização manual, com a refreshAPI:

 POST /_refresh 1
 POST /blogs/_refresh 2

 1 - Atualize todos os índices.

 2 - Atualize apenas o blogs índice.


 DICA
 Embora uma atualização seja muito mais leve que uma confirmação, ela ainda tem um custo de desempenho.
 Uma atualização manual pode ser útil ao escrever testes, mas não faça uma atualização manual sempre que indexar um documento em produção; vai prejudicar seu desempenho.
 Em vez disso, seu aplicativo precisa estar ciente da natureza quase em tempo real do Elasticsearch e fazer concessões para isso.

 Nem todos os casos de uso exigem uma atualização a cada segundo.
 Talvez você esteja usando o Elasticsearch para indexar milhões de arquivos de log e prefira otimizar a velocidade do índice em vez da pesquisa quase em tempo real.


 Você pode reduzir a frequência de atualizações por índice configurando o refresh_interval:

 PUT /my_logs
 {
   "settings": {
     "refresh_interval": "30s" 1
   }
 }


--------------Parte II. Pesquisar em profundidade---------------

A pesquisa não é apenas uma pesquisa de texto completo: uma grande parte dos seus dados serão valores estruturados, como datas e números.
Começaremos explicando como combinar pesquisa estruturadacom pesquisa de texto completo da maneira mais eficiente.

-----------Capítulo 12. Pesquisa Estruturada------------------

A pesquisa estruturada é sobre interrogardados que possuem estrutura inerente.
Datas, horas e números são todos estruturados: eles têm um formato preciso no qual você pode realizar operações lógicas.

---------------Encontrando valores exatos---------------------

Ao trabalhar com valores exatos,você estará trabalhando com filtros.
Os filtros são importantes porque são muito, muito rápidos.
Os filtros não calculam a relevância (evitando toda a fase de pontuação) e são facilmente armazenados em cache e devem ser usados com a maior frequência possível.

----------------Termo Filtro com Números-------------------

Vamos explorar o "term" filtro primeiro porque você vai usá-lo com freqüência.
Este filtro é capaz de lidar com números, booleanos, datas e texto.

Nosso objetivo é encontrar todos os produtos com um determinado preço.
Você pode estar familiarizado com SQL se estiver vindo de um banco de dados relacional.
Se expressássemos essa consulta como uma consulta SQL, ficaria assim:

SELECT document
FROM   products
WHERE  price = 20

Na DSL de consulta do Elasticsearch, usamos um termfiltro para realizar a mesma coisa. O termfiltro procurará o valor exato que especificamos.
Por si só, um term filtro é simples.
Ele aceita um nome de campo e o valor que desejamos encontrar:

{
    "term" : {
        "price" : 20
    }
}

O term filtro não é muito útil por si só, no entanto. Conforme discutido em “Query DSL” ,
a searchAPI espera um query, não um filter. Para usar nosso termfiltro,precisamos envolvê-lo com uma filtered consulta:

GET /my_store/products/_search
{
    "query" : {
        "filtered" : { 1
            "query" : {
                "match_all" : {} 2
            },
            "filter" : {
                "term" : { 3
                    "price" : 20
                }
            }
        }
    }
}

1
A filteredconsulta aceita a querye a filter.

2
A match_all é usado para retornar todos os documentos correspondentes. Este é o comportamento padrão, portanto, em exemplos futuros, simplesmente omitiremos a query seção.

3
O termfiltro que vimos anteriormente. Observe como ele é colocado dentro da filter cláusula.

Nas versões mais novas ficou assim:

{
  "query": {
    "bool": {
      "must": {
        "match_all": {} 1
      },
      "filter": {
        "term": {
          "user_id": "2"
        }
      }
    }
  },
  "_source": [
    "tweet" 2
  ]
}

1 = Aqui definimos que queremos sempre sem calcular o score.
2 = O campo _source tras somente os campos que vc quer, se colocar um campo que não existe traz nenhum.

Agora se mudarmos para o name:

{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "term": {
          "name": "John Smith"
        }
      }
    }
  },
  "_source": [
    "tweet"
  ]
}

Não traz resultado, porque é necessário informar caso o campo seja text o keyword
...
    "filter": {
        "term": {
          "name.keyword": "John Smith"
        }
...

Dessa forma vai trazer.

-----------------------------------Combinando Filtros-------------------------------

O bool filtro é composto por três seções:

{
   "bool" : {
      "must" :     [],
      "should" :   [],
      "must_not" : [],
   }
}

must
Todas essas cláusulas devem corresponder. O equivalente de AND.

must_not
Todas essas cláusulas não devem corresponder. O equivalente de NOT.

should
Pelo menos uma dessas cláusulas deve corresponder. O equivalente de OR.


Exemplo com should(or):

{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "bool": {
          "should": [
            {
              "term": {
                "date": "2014-09-24"
              }
            },
            {
              "term": {
                "name": "John Smith"
              }
            }
          ]
        }
      }
    }
  }
}



Exemplo com must(and):

{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "date": "2014-09-24"
              }
            },
            {
              "term": {
                "name.keyword": "John Smith"
              }
            }
          ]
        }
      }
    }
  }
}


Para reduzir pode ser utilizado em vez do match_all o constant_score, conforme exemplo:

{
  "query": {
    "constant_score": {
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "date": "2014-09-22"
              }
            },
            {
              "term": {
                "name.keyword": "John Smith"
              }
            }
          ]
        }
      }
    }
  }
 }

------------------Encontrando vários valores exatos----------------------------------

Em vez de usar vários termfiltros, você pode usar um único terms filtro (observe o s no final).
O terms filtro é simplesmente a versão plural do termfiltro singular.

Parece quase idêntico a uma baunilha termtambém. Em vez de especificar um único preço, agora estamos especificando uma matriz de valores:

{
    "terms" : {
        "price" : [20, 30]
    }
}


--------------------------Contém, mas não é igual--------------------------------------------

É importante entender que terme termssão contém operações, não é igual a . O que isso significa?

Se você tiver um filtro de termo para { "term" : { "tags" : "search" } }, ele corresponderá aos dois documentos a seguir:

{ "tags" : ["search"] }
{ "tags" : ["search", "open_source"] } 1
1
Este documento é devolvido, mesmo que tenha termos diferentes de search.

Lembre-se de como o termfiltro funciona: ele verifica o índice invertido de todos os documentos que contêm um termo e, em seguida, constrói um bitset. Em nosso exemplo simples, temos o seguinte índice invertido:

OBSERVAÇÃO
A natureza de um índice invertido também significa que toda a igualdade de campo é bastante difícil de calcular.
Como você determinaria se um documento específico contém apenas o termo de sua solicitação? Você teria que encontrar o termo no índice invertido, extrair os IDs do documento e, em seguida, varrer todas as linhas do índice invertido ,
procurando esses IDs para ver se um documento tem outros termos.

Como você pode imaginar, isso seria tremendamente ineficiente e caro.
Por essa razão, terme termssão devem conter operações, não devem ser exatamente iguais a .


-----------------Igual a Exatamente-----------------------------------------------------

Se você deseja esse comportamento - igualdade de campo inteiro - a melhor maneira de realizá-lo envolve a indexação de um campo secundário.
Nesse campo, você indexa o número de valores que seu campo contém.
Usando nossos dois documentos anteriores, agora incluímos um campo que mantém o número de tags:

{ "tags" : ["search"], "tag_count" : 1 }
{ "tags" : ["search", "open_source"], "tag_count" : 2 }

Depois de indexar as informações de contagem, você pode construir um bool filtro que aplique o número apropriado de termos:


{
    "query": {
        "filtered" : {
            "filter" : {
                 "bool" : {
                    "must" : [
                        { "term" : { "tags" : "search" } }, 1
                        { "term" : { "tag_count" : 1 } } 2
                    ]
                }
            }
        }
    }
}

1
Encontre todos os documentos que possuem o termo search.

2
Mas certifique-se de que o documento tenha apenas uma tag.

Essa consulta agora corresponderá apenas ao documento que possui uma única marca que é search, em vez de qualquer documento que contenha search.

---------------------Gamas-------------------------------

Até agora procuramos apenas números exatos. Na prática, filtrar por intervalos geralmente é mais útil.

"range" : {
    "price" : {
        "gt" : 20,
        "lt" : 40
    }
}

O rangefiltro suporta tanto intervalos inclusivos quanto exclusivos, por meio de combinações das seguintes opções:

gt: >maior que

lt: <menos de

gte: >=maior ou igual a

lte: <=menor ou igual a


----------------------Intervalos em datas--------------------------

O rangefiltro pode ser usado na datacampos também:

"range" : {
    "timestamp" : {
        "gt" : "2014-01-01 00:00:00",
        "lt" : "2014-01-07 00:00:00"
    }
}

Quando usado em campos de data, o rangefiltrosuporta operações matemáticas de data .
Por exemplo, se quisermos encontrar todos os documentos que tenham um carimbo de data/hora em algum momento da última hora:

"range" : {
    "timestamp" : {
        "gt" : "now-1h"
    }
}

A matemática de data também pode ser aplicada a datas reais, em vez de um espaço reservado como agora.
Basta adicionar um ( ||) após a data e segui-lo com uma expressão matemática de data:

"range" : {
    "timestamp" : {
        "gt" : "2014-01-01 00:00:00",
        "lt" : "2014-01-01 00:00:00||+1M" 1
    }
}

1
Menos de 1º de janeiro de 2014 mais um mês

--------------------------Lidando com valores nulos---------------------
Em última análise, isso significa que um null, [] (uma matriz vazia) e [null] são todos equivalentes.
O campo de tags é definido como nulo.

Eles simplesmente não existem no índice invertido!

Obviamente, o mundo não é simples, e os dados muitas vezes estão faltando campos, ou contêm
nulos explícitos ou matrizes vazias. Para lidar com essas situações, o Elasticsearch tem algumas
ferramentas para trabalhar com valores nulos ou ausentes.

-------------exists Filter-----------------------

A primeira ferramenta em seu arsenal é o filtro existente . Este filtro retornará documentos que
ter qualquer valor no campo especificado.

Em sql seria algo como:

SELECT tags
FROM posts
WHERE tags IS NOT NULL


No Elasticsearch, usamos o filtro existente :


"must": [
            {"exists" : { "field" : "date" }}
  ]

Os resultados são fáceis de entender. Qualquer documento que tenha termos no campo de date foi
retornado como uma ocorrência.

----------------Filtro ausente---------------------------

O filtro ausente é essencialmente o inverso de existe: ele retorna documentos onde não há valor para
um determinado campo, muito parecido com este SQL:

SELECT tags
FROM posts
WHERE tags IS NULL

Vamos trocar o filtro existente por um filtro ausente do nosso exemplo anterior:

"bool": {
          "must_not": [
             {"exists" : { "field" : "date" }}
          ]
        }

Obs: Quando nulo significa nulo

Às vezes, você precisa ser capaz de distinguir entre um campo que não tem um valor e um
campo que foi explicitamente definido como nulo. Com o comportamento padrão que vimos
anteriormente, isso é impossível; os dados são perdidos. Felizmente, existe uma opção que
podemos definir que substitui valores nulos explícitos por um valor de espaço reservado de nossa escolha.

Ao especificar o mapeamento para um campo de string, numérico, booleano ou de data, você
também pode definir um valor_nulo que será usado sempre que um valor nulo explícito for
encontrado. Um campo sem valor ainda será excluído do índice invertido.


----------------existe/ausente em objetos------------------------

Os filtros existentes e ausentes também funcionam em objetos internos, não apenas nos tipos principais.

você pode verificar a existência de name.first e name.last, mas também apenas name
Dissemos que um objeto como o anterior é nivelado internamente em uma estrutura de valor de campo simples, mais ou menos assim:


{
 "name.first" : "John",
 "name.last" : "Smith"
}

Então, como podemos usar um filtro existente ou ausente no campo de nome , que realmente não
existe no índice invertido?

A razão pela qual funciona é que um filtro como

{
 "exists" : { "field" : "name" }
}

é realmente executado como

{
 "bool": {
 "should": [
 { "exists": { "field": { "name.first" }}},
 { "exists": { "field": { "name.last" }}}
 ]
 }
 }

Isso também significa que se first e last estivessem vazios, o namespace seria
não existe

----Tudo sobre cache----------------------------

Em seu coração está um bitset representando quais documentos correspondem ao filtro.
O Elasticsearch armazena em cache esses bitssets de forma agressiva para uso posterior. Uma vez
armazenados em cache, esses conjuntos de bits podem ser reutilizados sempre que o mesmo filtro for usado,
sem a necessidade de reavaliar todo o filtro novamente.

Esses conjuntos de bits em cache são “inteligentes”: eles são atualizados de forma incremental. À medida que
você indexa novos documentos, apenas esses novos documentos precisam ser adicionados aos conjuntos de
bits existentes, em vez de ter que recalcular todo o filtro em cache repetidamente. Os filtros são em tempo real
como o resto do sistema; você não precisa se preocupar com a expiração do cache.


Cache de filtro independente

Cada filtro é calculado e armazenado em cache independentemente, independentemente de onde é usado. Se duas
consultas diferentes usarem o mesmo filtro, o mesmo bitset de filtro será reutilizado. Da mesma forma, se uma
única consulta usar o mesmo filtro em vários locais, apenas um bitset será calculado e reutilizado.


Vejamos esta consulta de exemplo, que procura e-mails que são um dos seguintes:

"bool": {
 "should": [
 { "bool": {
 "must": [
 { "term": { "folder": "inbox" }},1
 { "term": { "read": false }}
 ]
 }},
 { "bool": {
 "must_not": {
 "term": { "folder": "inbox" } 1
 },
 "must": {
 "term": { "important": true }
 }
 }}
 ]
 }

1 - Esses dois filtros são idênticos e usarão o mesmo bitset.

Embora uma das cláusulas de caixa de entrada seja uma cláusula must e a outra seja uma cláusula must_not , as
duas cláusulas são idênticas. Isso significa que o bitset é calculado uma vez para a primeira cláusula que é
executada e, em seguida, o bitset armazenado em cache é usado para a outra cláusula. Quando essa consulta é
executada pela segunda vez, o filtro da caixa de entrada já está armazenado em cache e, portanto, ambas as
cláusulas usarão o bitset armazenado em cache.

Isso combina muito bem com a capacidade de composição da consulta DSL. É fácil mover os filtros ou reutilizar o
mesmo filtro em vários locais na mesma consulta. Isso não é apenas conveniente para o desenvolvedor – tem
benefícios diretos de desempenho.


-------------Controlando o cache A maioria---------------

A maioria dos filtros folha — aqueles que lidam diretamente com campos como o termo filtro — são armazenados em cache,
enquanto os filtros compostos, como o filtro bool , não.

Os filtros folha precisam consultar o índice invertido no disco, então faz sentido armazená-los em cache. Os filtros compostos, por outro lado, usam
lógica de bits rápida para combinar os conjuntos de bits resultantes de suas cláusulas internas, por isso é eficiente recalcular sempre.

Certos filtros folha, no entanto, não são armazenados em cache por padrão, porque não faz sentido fazer isso:

Filtros de script:
Os resultados dos filtros de script não pode ser armazenado em cache porque o significado do script é opaco para o Elasticsearch.

Filtros de geolocalização:
Para filtrar resultados com base na geolocalização de um usuário específico

Intervalos de datas:
Intervalos de datas que usam a função now (por exemplo , "now-1h") resultam em valores com precisão
de milissegundos. Toda vez que o filtro é executado, agora retorna um novo horário.

Às vezes, a estratégia de cache padrão não está correta. Talvez você tenha uma expressão bool complicada
que é reutilizada várias vezes na mesma consulta. Ou você tem um filtro em um campo de data que nunca
será reutilizado. A estratégia de cache padrão pode ser substituída em quase qualquer filtro definindo o
sinalizador _cache :

{
    "range" : {
        "timestamp" : {
            "gt" : "2014-01-02 16:15:14" 1
        },
    "_cache": false Desative o cache deste filtro
    }
}

1 - É improvável que reutilizemos esse timestamp exato.

2 - Desative o cache deste filtro.

----------Order dos filtros----------------------------------

A ordem dos filtros em uma cláusula bool é importante para o desempenho. Filtros mais específicos devem
ser colocados antes de filtros menos específicos para excluir tantos documentos quanto possível, o mais
cedo possível.

Se a Cláusula A puder corresponder a 10 milhões de documentos e a Cláusula B puder corresponder apenas
a 100 documentos, a Cláusula B deverá ser colocada antes da Cláusula A.

GET /logs/2014-01/_search
{
    "query" : {
        "filtered" : {
            "filter" : {
                "range" : {
                    "timestamp" : {
                        "gt" : "now-1h"
                    }
                }
            }
        }
    }
}

Este filtro não é armazenado em cache porque usa a função now , cujo valor muda a cada milissegundo. Isso
significa que temos que examinar o valor de um mês de eventos de log toda vez que executarmos essa consulta!

Poderíamos tornar isso muito mais eficiente combinando-o com um filtro em cache: podemos excluir a maioria dos
dados do mês adicionando um filtro que usa um ponto fixo no tempo, como meia-noite da noite passada:

"bool": {
    "must": [
        { "range" : {
            "timestamp" : {
            "gt" : "now-1h/d" 1
        }
    }},
        { "range" : {
            "timestamp" : {
            "gt" : "now-1h" 2
        }
    }}
 ]
}

1 - Este filtro é armazenado em cache porque usa agora arredondado para meia-noite

2 - Este filtro não é armazenado em cache porque usa agora sem arredondamento.

A cláusula now-1h/d arredonda para a meia-noite anterior e, portanto, exclui todos os documentos criados antes de hoje.

O bitset resultante é armazenado em cache porque now é usado com arredondamento, o que significa que ele
é executado apenas uma vez por dia, quando o valor de meia-noite-última noite muda. A cláusula now-1h não é
armazenada em cache porque agora produz um tempo com precisão de milissegundos mais próximo. No entanto,
graças ao primeiro filtro, este segundo filtro precisa apenas verificar os documentos que foram criados desde a meianoite.

A ordem dessas cláusulas é importante. Essa abordagem funciona apenas porque a cláusula desde meia-noite vem
antes da cláusula de última hora. Se fosse o contrário, a cláusula de última hora precisaria examinar todos os
documentos no índice, em vez de apenas os documentos criados desde a meia-noite


----------------Pesquisa de texto completo--------------------------------

Como pesquisar em campos de texto completo para encontrar os documentos mais relevantes.


Os dois aspectos mais importantes da pesquisa de texto completo são os seguintes:

Relevância:

A capacidade de classificar os resultados de acordo com a relevância deles para a consulta fornecida, se
a relevância é calculada usando TF/IDF (consulte “O que é relevância?” na página 115), proximidade com
uma geolocalização, similaridade difusa ou algum outro algoritmo

Análise:

O processo de conversão de um bloco de texto em tokens normalizados distintos.

Assim que falamos sobre relevância ou análise, estamos no território das consultas, e não dos filtros.

------Baseado em Termo versus Texto Completo-----------

Embora todas as consultas executem algum tipo de cálculo de relevância, nem todas as consultas têm uma
fase de análise. Além de consultas especializadas como as consultas bool ou function_score , que não operam
em texto, as consultas textuais podem ser divididas em duas famílias:

Consultas baseadas em termos:

Consultas como o termo ou consultas difusas são consultas de baixo nível que não têm fase de análise.
Eles operam em um único termo. Uma consulta de termo para o termo que Foo procura esse termo exato no índice invertido
e calcula o _score de relevância TF/IDF para cada documento que contém o termo.

É importante lembrar que o termo query procura no índice invertido apenas o termo exato; ele não
corresponderá a nenhuma variante como foo ou FOO

Consultas de texto completo:

Consultas como match ou query_string são consultas de alto nível que entendem o mapeamento de um campo:

-Se você usá-los para consultar um campo de data ou inteiro , eles tratarão a string de consulta como uma data ou um inteiro, respectivamente.

-Se você consultar um campo de string de valor exato (not_analyzed) , eles tratarão toda a string de consulta como um único termo.

-Se você consultar um campo de texto completo (analisado) , eles passarão primeiro a string de consulta pelo analisador apropriado para produzir a lista de termos a serem consultados.


Se você quiser usar uma consulta em um campo de valor exato não_analisado, pense se realmente deseja uma consulta ou um filtro.

GET /_search

  "query": {
    "constant_score": {
        "filter": {
            "bool": {
                "must": [
                    {
                      "term": { "gender": "female" }
                    }
                ]
            }
        }
    }
}

-------------------A consulta de correspondência--------------------------

A consulta de correspondência é a consulta principal – a primeira consulta que você deve acessar sempre
que precisar consultar qualquer campo. É uma consulta de texto completo de alto nível, o que significa que
sabe como lidar com campos de texto completo e campos de valor exato.

Dito isso, o principal caso de uso para a consulta de correspondência é para pesquisa de texto completo. Então, vamos dar
uma olhada em como a pesquisa de texto completo funciona com um exemplo simples.

----------------Uma consulta de palavra única-----------------------------




GET /my_index/my_type/_search
{
 "query": {
    "match": {
        "title": "QUICK!"
        }
    }
}


O Elasticsearch executa a consulta de correspondência anterior da seguinte maneira:

1. Verifique o tipo de campo.

O campo de título é um campo de string de texto completo (analisado) , o que significa que a string de consulta
também deve ser analisada.

2. Analise a string de consulta

A string de consulta RÁPIDO! é passado pelo analisador padrão, o que resulta em um único termo rápido. Como
temos apenas um único termo, a consulta de correspondência pode ser executada como uma única consulta de
termo de baixo nível.

3. Encontre documentos correspondentes.

O termo consulta procura rapidamente no índice invertido.

4. Pontue cada doc.

O termo query calcula a relevância _score para cada documento correspondente, combinando a frequência do
termo (com que frequência quick aparece no campo de título de cada documento), com a frequência inversa do
documento (com que frequência quick aparece no campo de título em todos os documentos do index) e o
comprimento de cada campo (campos mais curtos são considerados mais relevantes)

"hits": [
{
 "_id": "1",
 "_score": 0.5, - 1
 "_source": {
 title": "The quick brown fox"
  }
 },
 {
  "_id": "3",
  "_score": 0.44194174,  - 2
  "_source": {
  "title": "The quick brown fox jumps over the quick dog"
  }
 },
 {
  "_id": "2",
  "_score": 0.3125, - 2
  "_source": {
  "title": "The quick brown fox jumps over the lazy dog"
  }
 }
 ]
 }


 O documento 1 é mais relevante porque seu campo de título é curto, o que significa que rápido representa uma
 grande parte de seu conteúdo.

 O documento 3 é mais relevante que o documento 2 porque rápido aparece duas vezes.


 ------------------Consultas de várias palavras----------------------

 Se pudéssemos pesquisar apenas uma palavra de cada vez, a pesquisa de texto completo seria bastante inflexível.
 Felizmente, a consulta de correspondência torna as consultas de várias palavras igualmente simples:

 GET /my_index/my_type/_search
 {
  "query": {
    "match": {
        "title": "BROWN DOG!"
        }
    }
 }


 A consulta anterior retorna todos os quatro documentos na lista de resultados:



 {
  "hits": [
  {
  "_id": "4",
  "_score": 0.73185337, 1
  "_source": {
  "title": "Brown fox brown dog"
  }
  },
  {
  "_id": "2",

  "_score": 0.47486103, 2
   "_source": {
   "title": "The quick brown fox jumps over the lazy dog"
   }
   },
   {
   "_id": "3",
   "_score": 0.47486103, 2
   "_source": {
   "title": "The quick brown fox jumps over the quick dog"
   }
   },
   {
   "_id": "1",
   "_score": 0.11914785, 3
   "_source": {
   "title": "The quick brown fox"
   }
   }
   ]
  }

   1 - O documento 4 é o mais relevante porque contém "marrom" duas vezes e "cachorro"

   2 - Os documentos 2 e 3 contêm brown e dog uma vez cada, e o campo de título tem o mesmo tamanho em
       ambos os documentos, portanto, eles têm a mesma pontuação

   3 - O documento 1 corresponde, embora contenha apenas marrom, não cachorro.

Como a consulta de correspondência precisa procurar dois termos—["brown","dog"]—internamente , ela precisa
executar duas consultas de termos e combinar seus resultados individuais no resultado geral. Para fazer isso,
ele agrupa as consultas de dois termos em uma consulta bool.

O importante a tirar disso é que qualquer documento cujo campo de título contenha pelo menos um dos termos
especificados corresponderá à consulta. Quanto mais termos corresponderem, mais relevante será o documento.


------------Melhorando a precisão A------------------

A correspondência de qualquer documento que contenha qualquer um dos termos de consulta pode resultar em
uma longa cauda de resultados aparentemente irrelevantes. É uma abordagem de espingarda para pesquisar.
Talvez queiramos mostrar apenas documentos que contenham todos os termos de consulta. Em outras
palavras, em vez de brown OR dog, queremos retornar apenas documentos que correspondam a brown AND dog.

GET /my_index/my_type/_search
{
    "query": {
        "match": {
            "title": { 1
                "query": "BROWN DOG!",
                "operator": "and"
            }
        }
    }
}

1 - A estrutura da consulta de correspondência deve mudar um pouco para acomodar o parâmetro
    do operador .

Essa consulta excluiria o documento 1, que contém apenas um dos dois termos.

----------------Controlando a precisão-----------------------

A escolha entre tudo e qualquer um é um pouco preto ou branco demais. E se o usuário especificar
cinco termos de consulta e um documento contiver apenas quatro deles? Definir operador para e excluiria este documento.

Às vezes, é exatamente isso que você deseja, mas para a maioria dos casos de uso de pesquisa de texto completo,
você deseja incluir documentos que possam ser relevantes, mas excluir aqueles que provavelmente não serão relevantes.
Em outras palavras, precisamos de algo intermediário.

A consulta de correspondência suporta o parâmetro Minimum_should_match , que permite
especificar o número de termos que devem corresponder para que um documento seja considerado
relevante. Embora você possa especificar um número absoluto de termos, geralmente faz sentido especificar uma porcentagem.

GET /my_index/my_type/_search
{
 "query": {
    "match": {
        "title": {
            "query": "quick brown dog",
            "minimum_should_match": "75%"
        }
    }
 }
}

Quando especificado como uma porcentagem, Minimum_should_match faz a coisa certa: no exemplo anterior com três termos, 75% seriam arredondados para 66,6%, ou dois dos três termos.
Independentemente do que você definir, pelo menos um termo deve corresponder para que um documento seja considerado uma correspondência.

O parâmetro Minimum_should_match é flexível e regras diferentes podem ser aplicadas dependendo do número de termos que o usuário inserir.

Para entender completamente como a consulta de correspondência lida com consultas de várias palavras,
precisamos ver como combinar várias consultas com a consulta bool .

----------Combinando consultas----------

Os filtros tomam uma decisão binária: este documento deve ou não ser incluído na lista de resultados? As
consultas, no entanto, são mais sutis. Eles decidem não apenas se devem incluir um documento, mas
também a relevância desse documento.
Os resultados da consulta anterior incluem qualquer documento cujo campo de título contenha o termo
quick, exceto aqueles que também contenham lazy. Até agora, isso é bastante semelhante ao modo como
o filtro bool funciona.
Como o filtro equivalente, a consulta bool aceita várias cláusulas de consulta nos parâmetros must,
must_not e should . Por exemplo:

GET /my_index/my_type/_search //
{
 "query": {
 "bool": {
 "must": { "match": { "title": "quick" }},
 "must_not": { "match": { "title": "lazy" }},
 "should": [
 { "match": { "title": "brown" }},
 { "match": { "title": "dog" }}
 ]
 }
 }
}

Os resultados da consulta anterior incluem qualquer documento cujo campo de título contenha o termo quick, exceto aqueles que também contenham lazy. Até agora, isso é bastante semelhante ao modo comoo filtro bool funciona.


A diferença vem com as duas cláusulas devem , que dizem que: um documento não precisa conter marrom ou cachorro, mas se contiver, deve ser considerado mais relevante:

{
 "hits": [
 {
 "_id": "3",
 "_score": 0.70134366, - 1
 "_source": {
  "title": "The quick brown fox jumps over the quick dog"
  }
  },
  {
  "_id": "1",
  "_score": 0.3312608,
  "_source": {
  "title": "The quick brown fox"
  }
  }
  ]
 }


1 - Documento 3 pontua mais alto porque contém marrom e cachorro.


-------Cálculo de pontuação--------------

A consulta booleana calcula o _score de relevância para cada documento adicionando o _score de todas as cláusulas must e must correspondentes e , em seguida, dividindo pelo número total de cláusulas must e should.

As cláusulas must_not não afetam a pontuação; sua única finalidade é excluir documentos que de outra forma poderiam ter sido incluídos.

-------Controlando a Precisão-----------

Todas as cláusulas must devem corresponder e todas as cláusulas must_not não devem corresponder, mas quantas cláusulas devem corresponder ? Por padrão, nenhuma das cláusulas devem corresponder,
com uma exceção: se não houver cláusulas devem , pelo menos uma cláusula deve corresponder.

Assim como podemos controlar a precisão da consulta de correspondência , podemos controlar quantas cláusulas devem corresponder usando o parâmetro Minimum_should_match ,
seja como um número absoluto ou como uma porcentagem:


GET /my_index/my_type/_search
{
 "query": {
 "bool": {
 "should": [
 { "match": { "title": "brown" }},
 { "match": { "title": "fox" }},
 { "match": { "title": "dog" }}
 ],
 "minimum_should_match": 2 - 1
 }
 }
}

1 - Isso também pode ser expresso em porcentagem.

Os resultados incluiriam apenas documentos cujo campo de título contenha "marrom" E "raposa", "marrom" E
"cachorro" ou "raposa" E "cachorro". Se um documento contiver todos os três, será considerado mais relevante do que aqueles que contenham apenas dois dos três.

-----Como o jogo usa bool------------

Até agora, você provavelmente já percebeu que as consultas de correspondência de várias palavras simplesmente
envolvem as consultas de termo geradas em uma consulta bool . Com o operador ou padrão , cada consulta de termo
é adicionada como uma cláusula should , portanto, pelo menos uma cláusula deve corresponder. Essas duas consultas
são equivalentes:

{
 "match": { "title": "brown fox"}
}

{
 "bool": {
 "should": [
 { "term": { "title": "brown" }},
 { "term": { "title": "fox" }}
 ]
 }
}

Com o operador and , todas as consultas de termos são adicionadas como cláusulas must , portanto, todas as
cláusulas devem corresponder. Essas duas consultas são equivalentes:

{
 "match": {
 "title": {
 "query": "brown fox",
 "operator": "and"
 }
 }
}

{
 "bool": {
 "must": [
 { "term": { "title": "brown" }},
 { "term": { "title": "fox" }}
 ]
 }
}

E se o parâmetro Minimum_should_match for especificado, ele é passado diretamente para a consulta bool , tornando
essas duas consultas equivalentes


{
 "match": {
 "title": {
 "query": "quick brown fox",
 "minimum_should_match": "75%"
 }
 }
}
{
 "bool": {
 "should": [
 { "term": { "title": "brown" }},
 { "term": { "title": "fox" }},
 { "term": { "title": "quick" }}
 ],
 "minimum_should_match": 2 -1
 }
}

1 - Como há apenas três cláusulas, o valor Minimum_should_match de 75% na consulta de correspondência é
arredondado para 2. Pelo menos duas das três cláusulas devem corresponder.

É claro que normalmente escreveríamos esses tipos de consultas usando a consulta de correspondência , mas entender
como a consulta de correspondência funciona internamente permite que você assuma o controle do processo quando
precisar. Algumas coisas não podem ser feitas com uma única consulta de correspondência , como dar mais peso a
alguns termos de consulta do que a outros. Veremos um exemplo disso na próxima seção.


---------------------Impulsionando cláusulas de consulta--------------

É claro que a consulta bool não se restringe a combinar consultas simples de correspondência de uma palavra . Ele
pode combinar qualquer outra consulta, incluindo outras consultas booleanas . É comumente usado para ajustar a
_score de relevância para cada documento combinando as pontuações de várias consultas distintas.


Imagine que queremos pesquisar documentos sobre “pesquisa de texto completo”, mas queremos dar mais peso a
documentos que também mencionam “Elasticsearch” ou “Lucene”. Por mais peso, queremos dizer que documentos que
mencionam “Elasticsearch” ou “Lucene” receberão um _score de relevância maior do que aqueles que não o fizerem, o
que significa que eles aparecerão mais alto na lista de resultados.

Uma simples consulta bool nos permite escrever essa lógica bastante complexa da seguinte forma:

GET /_search
{
 "query": {
 "bool": {
  "must": {
  "match": {
  "content": { 1
  "query": "full text search",
  "operator": "and"
  }
  }
  },
  "should": [ 2
  { "match": { "content": "Elasticsearch" }},
  { "match": { "content": "Lucene" }}
  ]
  }
 }
 }


 1 - O campo de conteúdo deve conter todas as palavras completa, texto e pesquisa.

 2 - Se o campo de conteúdo também contiver Elasticsearch ou Lucene, o documento receberá
     um _score mais alto.

Mas e se quisermos dar mais peso aos documentos que contêm Lucene e ainda mais peso aos
documentos que contêm Elasticsearch?

Podemos controlar o peso relativo de qualquer cláusula de consulta especificando um valor de
reforço , cujo padrão é 1. Um valor de reforço maior que 1 aumenta o peso relativo dessa cláusula.
Assim, poderíamos reescrever a consulta anterior da seguinte forma:


GET /_search
{
 "query": {
 "bool": {
 "must": {
 "match": { - 1
 "content": {
 "query": "full text search",
 "operator": "and"
 }
 }
 },
 "should": [
 { "match": {
 "content": {
 "query": "Elasticsearch",
 "boost": 3 - 2
 }
 }},
 { "match": {
    "content": {
     "query": "Lucene",
     "boost": 2 - 3
     }
     }}
     ]
     }
     }
 }

1 - Essas cláusulas usam o aumento padrão de 1

2 - Esta cláusula é a mais importante, pois tem o maior impulso

3 - Essa cláusula é mais importante que o padrão, mas não tão importante quanto a cláusula Elasticsearch.

O parâmetro boost é usado para aumentar o peso relativo de uma cláusula (com um boost maior que 1) ou diminuir o peso relativo (com um boost entre 0 e 1),
mas o aumento ou diminuição não é linear. Em outras palavras, um aumento de 2 não resulta no dobro de pontuação.

Em vez disso, o novo _score é normalizado após a aplicação do aumento. Cada tipo de consulta tem seu próprio algoritmo de normalização e os
detalhes estão além do escopo deste livro. Basta dizer que um valor de aumento mais alto resulta em um _score mais alto.

Se você estiver implementando seu próprio modelo de pontuação não baseado em TF/IDF e precisar de mais controle sobre o processo de
aumento, poderá usar a consulta function_score para manipular o aumento de um documento sem a etapa de normalização.

-------------------Análise de controle---------------------- //

As consultas podem encontrar apenas os termos que realmente existem no índice invertido, portanto, é
importante garantir que o mesmo processo de análise seja aplicado ao documento no momento do índice
e à string da consulta no momento da pesquisa para que os termos da consulta correspondam os termos
no índice invertido.

Embora digamos documento, os analisadores são determinados por campo. Cada campo pode ter um
analisador diferente, seja configurando um analisador específico para aquele campo ou caindo de volta aos padrões de tipo, índice ou nó. Na hora do índice,
o valor de um campo é analisado usando o analisador configurado ou padrão para esse campo.

PUT /my_index/_mapping/my_type
{
 "my_type": {
 "properties": {
 "english_title": {
 "type": "string",
 "analyzer": "english"
 }
 }
 }
}


GET /my_index/_analyze?field=my_type.title 1
Foxes
GET /my_index/_analyze?field=my_type.english_title 2
Foxes

1 - O título do campo , que usa o analisador padrão padrão , retornará o termo raposas

2 - O campo english_title, que utiliza o analisador inglês , retornará o termo fox.

Isso significa que, se executarmos uma consulta de termo de baixo nível para o termo exato fox, o campo
english_title corresponderia, mas o campo title não.

As consultas de alto nível, como a consulta de correspondência, entendem os mapeamentos de campo e podem aplicar o analisador correto para cada campo consultado. Podemos ver isso em ação com a
API de consulta de validação:

GET /my_index/my_type/_validate/query?explain
{
 "query": {
 "bool": {
 "should": [
 { "match": { "title": "Foxes"}},
 { "match": { "english_title": "Foxes"}}
 ]
 }
 }
}

Isso significa que, se executarmos uma consulta de termo de baixo nível para o termo exato fox, o campo
english_title corresponderia, mas o campo title não.

As consultas de alto nível, como a consulta de correspondência, entendem os mapeamentos de campo e podem aplicar o analisador correto para cada campo consultado.
Podemos ver isso em ação com a API de consulta de validação:


GET /my_index/my_type/_validate/query?explain
{
 "query": {
 "bool": {
 "should": [
 { "match": { "title": "Foxes"}},
 { "match": { "english_title": "Foxes"}}
 ]
 }
 }
}

A consulta de correspondência usa o analisador apropriado para cada campo para garantir que ela procure cada termo no formato correto para esse campo.


---------------Analisadores padrão---------------------


Embora possamos especificar um analisador no nível do campo, como determinamos qual analisador
é usado para um campo se nenhum for especificado no nível do campo?

Os analisadores podem ser especificados em vários níveis. O Elasticsearch trabalha em cada nível até encontrar um analisador que possa ser usado. Na hora do índice, a ordem é a seguinte:

• O analisador definido no mapeamento de campo, senão
• O analisador definido no campo _analyzer do documento, senão
• O analisador padrão para o tipo, cujo padrão é
• O analisador denominado default nas configurações de índice, cujo padrão é
• O analisador denominado default no nível do nó, que tem como padrão
• O analisador padrão

Na hora da busca, a sequência é um pouco diferente:

• O analisador definido na própria consulta, senão
• O analisador definido no mapeamento de campo, senão
• O analisador padrão para o tipo, cujo padrão é
• O analisador denominado default nas configurações de índice, cujo padrão é
• O analisador denominado default no nível do nó, que tem como padrão
• O analisador padrão

-------------Conguring Analyzers in Practice------------------


Antes vamos fazer um rápido desvio para explicar por que criamos nosso índice de teste com apenas um shard primário.

De vez em quando, um novo usuário abre um problema alegando que a classificação por relevância está quebrada e oferece uma reprodução curta:
o usuário indexa alguns documentos, executa uma consulta simples e encontra resultados aparentemente menos relevantes aparecendo acima de resultados mais relevantes.

Para entender por que isso acontece, vamos imaginar que criamos um índice com dois shards primários
e indexamos dez documentos, seis dos quais contêm a palavra foo. Pode acontecer que o fragmento 1
contenha três dos documentos foo e o fragmento 2 contenha os outros três. Em outras palavras, nossos
documentos são bem distribuídos.

A frequência do termo conta o número de vezes que um termo aparece no campo que estamos
consultando no documento atual. Quanto mais vezes aparecer, mais relevante é este documento. A
frequência inversa de documentos leva em consideração a frequência com que um termo aparece
como uma porcentagem de todos os documentos no índice. Quanto mais frequentemente o termo
aparece, menos peso ele tem.

No entanto, por motivos de desempenho, o Elasticsearch não calcula o IDF em todos os documentos
no índice. Em vez disso, cada fragmento calcula um IDF local para os documentos contidos nesse
fragmento.

Como nossos documentos são bem distribuídos, o IDF para ambos os estilhaços será o mesmo.

Agora imagine que cinco dos documentos foo estão no fragmento 1 e o sexto documento está no
fragmento 2. Nesse cenário, o termo foo é muito comum em um fragmento (e, portanto, de pouca
importância), mas raro no outro fragmento (e muito mais importante). Essas diferenças no IDF podem
produzir resultados incorretos.

Na prática, isso não é um problema. As diferenças entre IDF local e global diminuem quanto mais
documentos você adiciona ao índice. Com volumes de dados do mundo real, os IDFs locais logo se
equilibram. O problema não é que a relevância está quebrada, mas que há poucos dados.

Para fins de teste, há duas maneiras de contornar esse problema. A primeira é criar um índice com um
fragmento primário, como fizemos na seção que apresenta a consulta de correspondência . Se você
tiver apenas um estilhaço, o IDF local será o IDF global.

A segunda solução é adicionar ?search_type=dfs_query_then_fetch às suas solicitações de pesquisa.
O dfs significa Distributed Frequency Search e diz ao Elasticsearch para primeiro recuperar o IDF local
de cada fragmento para calcular o IDF global em todo o índice.

--Dica:

Não use dfs_query_then_fetch em produção. Realmente não é
necessário. Apenas ter dados suficientes garantirá que suas frequências
de semestre sejam bem distribuídas. Não há motivo para adicionar essa
etapa DFS extra a todas as consultas executadas.

--------------Pesquisa de vários campos-------------------------------


Frequentemente, precisamos pesquisar as mesmas ou diferentes strings de consulta em um ou mais campos, o que significa
que precisamos combinar várias cláusulas de consulta e suas pontuações de relevância de uma maneira que faça sentido.

Talvez estejamos procurando um livro chamado Guerra e paz de um autor chamado Leo Tolstói. Talvez
estejamos pesquisando na documentação do Elasticsearch por “mínimo deve corresponder”, que pode
estar no título ou no corpo de uma página. Ou talvez estejamos procurando usuários com nome John e
sobrenome Smith.



-------------Diferença entre "match" e "terms" -----------------

Match trata de valores apróximados.

e term para valores exatos, cuidado com campos que possuem 2 tipificação.

-----------Várias strings de consulta---------------------------

A consulta multicampo mais simples de se lidar é aquela em que podemos mapear termos de pesquisa
para campos específicos. Se sabemos que Guerra e Paz é o título e Leo Tolstoy é o autor, é fácil escrever
cada uma dessas condições como uma cláusula de correspondência e combiná-las com uma consulta
bool:

GET /_search
{
 "query": {
 "bool": {
 "should": [
 { "match": { "title": "War and Peace" }},
 { "match": { "author": "Leo Tolstoy" }}
 ]
 }
 }
} //TODO 218 ajustar consulta e fazer...